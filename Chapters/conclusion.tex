\chapter{Conclusions and Future Work} \label{chapter:conclusions}

This chapter summarizes the main findings of this work and provides conclusions on the project in Section \ref{sec:conclu}. Based on these conclusions, Section \ref{sec:reco} puts forth recommendations for future work.

\section{Conclusions} \label{sec:conclu}

This report investigated the transmission of deep feature tensors over an imperfect channel. It also discussed a proposed tensor completion method, \gls{caltec}, which was specifically developed to `repair' corrupted deep feature tensors in \glslink{ci}{Collaborative Intelligence}. Furthermore, the report benchmarked the peformance of the proposed method against two general tensor completion methods which are widely used in computer vision.

More specifically, the contributions are as follows:

\begin{itemize}
	\item The effect of quantizing deep feature tensors in \glslink{dnn}{deep models} with skip connections.\\
	A \gls{resnet18} was split at six layers in different experiments. The transmitted tensors were tested over a range of quantization resolutions ($\{2,4,6,8,10\}$ bits). This work reports on the cloud's prediction accuracy on these quantized tensors. Of these six layers of \gls{resnet18}, three involved two distinct tensors being transmitted from the edge sub-model to the cloud sub-model. This work reports on investigating the end effect of quantizing the pair of tensors at different resolutions. It was shown that one tensor is less significant than the other in terms of its effect on the cloud prediction accuracy. Therefore, the cloud sub-model is more resilient to quantization errors in one tensor than in the other. This leads to the idea that it may be possible to use different quantization resolutions for the transmitted tensors (thereby reducing the `volume' of data to be transmitted over the communication channel) while ensuring an acceptable cloud prediction accuracy level.
	\item The effect of Gilbert-Elliott channel parameters on \gls{resnet18} tensors from different split layers. \\
	This work reports on studying the effect of burst loss channel parameters on the cloud model's Top-1 prediction accuracy. Transmission over Gilbert-Elliott channels lead to missing packets in deep feature tensors. Since \gls{resnet18} deep feature tensors typically contain few zeros, they are very sensitive to packet loss. This means that the cloud's prediction accuracy is strongly affected by the packet loss introduced by Gilbert-Elliott channels. Furthermore, it was shown that split layers which lead to two transmitted deep feature tensors have a low tolerance for the packet loss caused by the Gilbert-Elliott channel.
	\item The effect of `completing' damaged deep feature tensors with \gls{silrtc}, \gls{halrtc} and \gls{caltec} in \gls{dfts} simulations. \\
	This work shows the improvement in cloud prediction performance when tensor completion methods are applied to corrupted \gls{resnet18} deep feature tensors. It was shown that the cloud prediction accuracy improves significantly as the number of iterations of \gls{silrtc} is increased from 50 to 200 iterations. On the other hand, there was negligible improvement in increasing the number of iterations of \gls{halrtc} beyond 50 in our experiments.
	\item A proposed tensor completion which has a reduced computational cost.\\
	This work has put forth a proposed tensor completion, \gls{caltec}, which is very simple and requires much less execution time than two methods from the literature. In all simulation scenarios, \gls{caltec} provided very similar cloud prediction performance to methods from the literature while requiring less time to execute.
	\item \gls{caltec} outperforms \gls{silrtc} in all simulation scenarios while having a lesser impact on inference latency. \\
	The proposed method \gls{caltec} leads to a higher cloud prediction accuracy than \gls{silrtc} in all scenarios investigated in this work. \gls{caltec} outperforms \gls{silrtc} by up to $16\%$ (when $\gls{pb}=30\%$) in our Monte Carlo experiments. In all scenarios considered, \gls{caltec} took less than 15 minutes to process one Monte Carlo run's worth of missing packets. On the other hand, \gls{silrtc} took more than 2 hours in compute jobs with identical hardware configurations. 
	\item In high burst loss probability ($\gls{pb} = 20\%, 30\%$) scenarios, \gls{caltec} outperforms \gls{halrtc}. \\
	When averaged over all average burst lengths, our results show that the proposed method outperforms \gls{halrtc} by up to $1.46\%$ (when $\gls{pb}=30\%$). \gls{caltec} achieves this better performance while having a lesser impact on inference latency. \gls{halrtc} always took more than 2 hours to process a single Monte Carlo run's worth of corrupted tensors. In contrast, \gls{caltec} always took less than fifteen minutes to process the exact same corrupted tensors.
	\item \gls{halrtc} outperforms \gls{caltec} in low burst loss probability ($\gls{pb}=1\%,10\%$) scenarios. \\
	\gls{halrtc} offers an improvement of less than 1\% over the proposed method in experiments with low burst loss probability. A larger number of correctly received deep feature tensor values are available to exploit in order to recover the missing values. This is beneficial to both \gls{halrtc} and \gls{caltec}. However, this outlines the limitation of \gls{caltec} in transforming candidate packets into recovered packets. This means that \gls{caltec}-recovered packets are less accurate than \gls{halrtc}-recovered packets. It should be noted that \gls{caltec} manages to achieve very close results to \gls{halrtc} while having a reduced impact on inference latency.
	\item \gls{caltec} leads to the best cloud prediction accuracy in speed-matched experiments. \\
	In these Monte Carlo experiments, \gls{caltec} set an execution time baseline and the other two methods were run until their execution time just exceeded \gls{caltec}'s. This resulted in a single iteration of \gls{silrtc} and \gls{halrtc} being completed before the cloud sub-model prediction was done. \gls{halrtc}-repaired tensors led to cloud prediction accuracies which were lower than in the \textbf{NC} scenario (Gilbert-Elliot channel with no tensor completion). \gls{silrtc}-repaired tensors led to cloud prediction accuracies which were close to or worse than the in the \textbf{NC} scenario.
\end{itemize}

The proposed method compares favorably with other tensor completion methods.
\begin{itemize}
	\item \gls{caltec} executes extremely fast compared to \gls{silrtc} and \gls{halrtc}. \\
	\gls{caltec} is always at least 8 times faster than \gls{silrtc} and \gls{halrtc} when presented with the same corrupted deep feature tensors. This means that \gls{caltec} may be the better choice in time-critical \gls{ci} applications. \gls{caltec}'s superiority was further confirmed by the results of the speed-matched experiments.
	\item \gls{caltec}-repaired tensors assist the cloud in completing the inference task. \\ 
	In high burst loss probability scenarios, \gls{caltec} leads to the best observed prediction accuracies. In lower burst lost probability scenarios, \gls{caltec} produces very similar prediction accuracies (less than $1\%$ difference) as \gls{halrtc}.
	\item \gls{caltec} does not require any prior training, unlike \gls{altec}. \\
	While there are no quantitative results comparing the performance of the proposed method and \gls{altec} \cite{9017944}, \gls{caltec} has the advantage of not requiring any prior training, which means it is less costly to implement. It also implies that \gls{caltec} can be applied at any arbitrary split layer of a deep model. On the other hand, \gls{altec} requires training for each layer where the \gls{dnn} is split. Furthermore, \gls{caltec} does not require weights to be saved prior to its online operation.
	\item \gls{caltec} is content-adaptive and therefore should generalize well out of sample. \\ Unlike \gls{altec} which was trained on an \textit{Imagenet} validation set, \gls{caltec} is not trained on a dataset. Therefore, it is more likely to generalize better to out-of-sample data than \gls{altec}.
\end{itemize}

Finally, \gls{caltec}'s packet recovery results suggest that there is some level of redundancy in channels in a deep feature tensor from \gls{resnet18}'s \addone~layer. 

%Regarding the updated \gls{dfts} software, the following contributions were made:
%\begin{itemize}
%	\item \gls{dfts} has been updated to be compatible with TensorFlow 2. \\
%	Since \gls{dfts} is now compatible with the latest version of TensorFlow, it can be used to experiment with state of the art deep models. The new \gls{dfts} is also now in a better position to be adapted to run object detection experiments.
%	\item \gls{dfts} can now quantize tensors produced by the device sub-model at different resolutions.
%	\item \gls{dfts} experiments can use tensor completion methods to repair corrupted deep feature tensors.
%	\item 
%\end{itemize}


\section{Recommendations} \label{sec:reco}

%To further investigate the benefits of \gls{caltec}. it is recommended to run Monte Carlo experiments under `speed-matched' settings. Following the approach in \cite{9017944}, the proposed method is first run over a corrupted tensor and its execution time is recorded. Then \gls{halrtc} will be run iteratively on the same damaged tensor until the execution time has reached that of \gls{caltec}. This will give a better idea of the benefits of the proposed method over \gls{halrtc} in repairing corrupted deep feature tensors in a \gls{ci} context. Moreover, 
It may be interesting to train an \gls{altec} and compare its performance against \gls{caltec}. Since \gls{altec} requires training, it was not feasible to train it for evaluation in this work. 

Considering that \gls{caltec} has shown that some channels in a deep feature tensor are highly correlated with others and that there may be redundancy in some channels, there may be some benefit to learning the relationships between channels of a deep feature tensor produced at a given split layer of a deep model. Then this learned correlation may be exploited to recover missing packets in a corrupted tensor when operating online.

It is also worthwhile to investigate the effect of tensor completion in \gls{dfts} experiments in which two deep feature tensors are transmitted over a Gilbert-Elliott channel. As suggested by the quantization experiment results on \gls{resnet18} tensors, there may be some benefit in investigating completing only one of the two corrupted tensors given their unequal importance in cloud prediction performance.

Finally, now that \gls{dfts} is compatible with TensorFlow version 2, future work may extend it to run object detection tasks. The latter may find real life applications in the context of \gls{ci}, particularly in surveillance.