\chapter{Literature Review} \label{chapter:background}
This chapter serves as a review of the literature relevant to this project. It firstly provides background information, in Section \ref{sec:background:dnn}, on deep neural networks. Then Section \ref{sec:background:ci} discusses the theory behind collaborative intelligence. Based on the discussion in these two sections, the following section \ref{sec:background:dfts} gives a detailed overview of the original \gls{dfts} software. Section \ref{sec:background:tc} introduces the two general tensor completion methods, namely \gls{silrtc} and \gls{halrtc}, employed in this project. Finally, Section \ref{sec:background:summary} summarizes the key aspects of the literature review and relates them to subsequent work chapters.

\section{Deep Neural Networks} \label{sec:background:dnn}
Since a deep Convolutional Neural Network \cite{KrizhevskySH12} won the 2012 ImageNet Large-Scale Visual Recognition Challenge\footnote{\url{http://www.image-net.org/}}, Convolutional Neural Networks, also called \gls{cnn}s or ConvNets, have become the \textit{de facto} approach in image classification research. ConvNets achieve state-of-the-art results in large-scale image and video recognition tasks. Representation depth is a key factor in classification accuracy. A popular \gls{cnn} used for image classification is \gls{vgg16} \cite{vgg16}. Even though it is no longer a state of the art deep model, \gls{vgg16} is used in some experiments reported in this work. Simonyan and Zisserman \cite{vgg16} demonstrated that increasing the number of weight layers in a deep model leads to significant improvements in classification performance. However, with deeper models, a degradation phenomenon occurs when training the model: the training accuracy saturates and then degrades as the number of training epochs increase \cite{he2016deep}. Residual networks were developed to incorporate a number of residual blocks which implement a skip or shortcut connection between stacked layers \cite{he2016deep}. \gls{resnet18} is a popular residual network which is used in this work. Even though \gls{resnet18} is no longer a state of the art \gls{dnn}, it is still employed in this work because it is commonly used in deep learning research.

\section{Collaborative Intelligence} \label{sec:background:ci}

As mentioned in Chapter \ref{chapter:intro}, collaborative intelligence is an artificial intelligence deployment strategy that leverages edge-based and cloud-based compute resources to execute inference tasks faster and more efficiently \cite{jointdnn} \cite{neurosurgeon}. It involves a deep model being split into an edge sub-model and a cloud sub-model with intermediate deep feature tensors being transmitted over a communication channel from the edge (mobile device) to the cloud. These deep feature tensors may be corrupted by the unreliable communication channel. Recovering these missing deep feature tensor values is a new problem to the deep learning research community. In a \glslink{ci}{Collaborative Intelligence} context, Unnibhavi \cite{unnibhavi2018dfts} developed the \glslink{dfts}{Deep Feature Transmission Simulator} to study the effect of unreliable communication channels on deep feature tensor transmission. Section \ref{sec:background:dfts} delves deeper into this simulator. Bragilevsky in \cite{9017944} reports on developing a new tensor completion method, \gls{altec}, to repair deep feature tensor rows under a random loss channel for an image classification task in a \gls{ci} context. Section \ref{sec:background:tc} delves deeper into the literature on tensor completion methods.

\section{Deep Feature Transmission Simulator} \label{sec:background:dfts}

The deep feature transmission simulator (\gls{dfts}) was developed to simulate packet-based transmission of deep feature tensors over unreliable communication channels \cite{unnibhavi2018dfts}. \gls{dfts} simulations can include a Gilbert-Elliott model to simulate a communication channel. The burst loss model or Gilbert-Elliott model has been shown to fittingly capture real-time observed packet loss patterns over the Internet \cite{5755057}. The remote client, that is the cloud service in a \gls{ci} context, identifies missing packets from packet sequence numbers provided by a transport-layer protocol. Furthermore, \gls{dfts} simulations can be initialized to emulate quantization of deep feature tensor packets before transmission over a communication channel. The original \gls{dfts} provides error concealment methods such as nearest neighbor interpolation or linear interpolation to handle missing packets in a corrupted deep feature tensor \gls{xda}. Originally developed in TensorFlow 1.1.2 and Keras 2.2.2, \gls{dfts} was able to split a deep model meant for classification tasks. Even though the original \gls{dfts} was intended to be compatible with the object detection task, the write up \cite{unnibhavi2018dfts} does not report on tasks other than image classification. The original version of \gls{dfts} is not compatible with the latest TensorFlow library (versions 2.2 and 2.3), which is a significant drawback for its use in developing new simulation configurations.

\section{Tensor completion methods} \label{sec:background:tc}

Tensor completion methods refer to techniques which attempt to recover missing values in generic tensors regardless of application area. To our knowledge, there is no publication on tensor completion methods being applied to corrupt tensors in a \gls{ci} framework, apart from \cite{9017944}. General tensor completion methods have been developed in a computer vision application to recover missing values in images \cite{liu2012tensor}. Two of these methods, \gls{silrtc} and \gls{halrtc}, have been successfully employed to repair corrupted deep feature tensors in a collaborative intelligence application in \cite{9017944}. \gls{silrtc} stands for Simple Low Rank Tensor Completion while \gls{halrtc} stands for High accuracy Low Rank Tensor Completion. Both of these general tensor completion methods assume that the original, un-corrupted tensors are low-rank. Therefore, they iteratively reduce the rank of the damaged tensor with the assumption that this will lead to recovering the original tensor values. Due to their iterative operation, these general tensor completion methods introduce significant processing delays, which is a problem in applications which are sensitive to inference latency. \gls{silrtc} and \gls{halrtc} make no assumption as to the spatial distribution of missing values in a corrupted tensor so long as the indices of the correct tensor values are known. These two approaches are therefore a good choice of tensor completion method for benchmarking against our proposed method. Their algorithms are shown in Appendix \ref{appendix:a}.

In a collaborative intelligence application in which the inference task was also image classification, a deep feature tensor-specific method called \gls{altec} was proposed in \cite{9017944}. \gls{altec} stands for Adaptive Linear Tensor Completion. For a given split layer of a \gls{dnn}, it is pre-trained on a dataset. A set of weights are learned for each row in each channel of a deep feature tensor \gls{xtensor} for a given split layer. These weights are used to recover a deep feature tensor row by doing a weighted combination of its spatial neighbors in-channel and its co-located rows in other channels. \gls{altec} was not benchmarked in a \gls{dfts} simulation environment. Since \gls{altec} requires training for each layer at which the \gls{dnn} is split in a \gls{ci} framework, developing an implementation of \gls{altec} may take a potentially excessive amount of time. Therefore, \gls{altec} is not tested in this work. The assumptions behind \gls{altec} have been shown to be correct given the results reported in \cite{9017944}, therefore these assumptions may be used to guide the development of the new proposed tensor completion method.

\section{Summary} \label{sec:background:summary}

This chapter has reviewed the literature on topics relevant to this study. A key point is that the original \gls{dfts} would benefit from an upgrade to become TensorFlow 2-compatible. Furthermore, \gls{silrtc} and \gls{halrtc} will be used for benchmarking purposes in this work. The rationale behind \gls{altec} will be used to derive a proposed tensor completion method in Chapter \ref{chapter:caltec}.