\chapter{Experiments and Results} \label{chapter:expts}

\section{Introduction} \label{sec:expts:intro}
This chapter discusses in detail experiments performed in this project. It begins by outlining the simulation environments used in Section \ref{sec:expts:details}. Section 
\ref{sec:expts:dnn} explains how experiments are prepared and run in \gls{dfts}2. Sections \ref{sec:expts:quant} and \ref{sec:expts:channel} investigate the effect on cloud classification accuracy of quantizing the transmitted tensors and of varying the imperfect communication channel's characteristics. Section \ref{sec:expts:repair} illustrates how tensor completion methods repair damaged deep feature tensor channels. Section \ref{sec:expts:iter} discusses simulations run to find the minimum suitable iteration count for the two general tensor completion methods (\gls{silrtc} and \gls{halrtc}). Based on the chosen minimum suitable iteration count, these tensor completion methods are presented with damaged tensor data from Monte Carlo experiments in Section \ref{sec:expts:default} and they are benchmarked against the proposed method. Speed-matched experiments are investigated in Section \ref{sec:expts:speed}. Finally, the main experimental results are summarized in Section \ref{sec:expts:summary}.

\section{Experiment details} \label{sec:expts:details}
Two pre-trained deep models widely used for image classification were employed in this work: \gls{vgg16} and \gls{resnet18}. \gls{vgg16} was successfully used in the development of the original \gls{dfts} simulator. \gls{resnet18} is a more modern \gls{dnn} featuring skip connections (i.e. a residual network), of which the 34-layer variant ResNet34 has been used in similar \gls{ci} study \cite{9017944}.

\gls{dfts}2 was developed in Python 3.6 and TensorFlow 2.2 on an Ubuntu 18.04 machine. Monte Carlo experiments were run on ComputeCanada clusters (Cedar and Graham) in Python 3.6 and TensorFlow 2.3 virtual environments.

The test set utilized in this project was obtained from the original \gls{dfts}'s GitHub repository. It is constituted of 882 images from ten classes of the Imagenet validation set.

\section{Splitting a DNN} \label{sec:expts:dnn}
We consider the case in which a \gls{vgg16} is split into an edge sub-model and a cloud sub-model at the output of the \vggblock~layer, which we refer to as the split layer.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.34]{Figures/vgg16splitblock3conv2.pdf}
	\caption[VGG16 split layer]{A \gls{vgg16} split at the output of \vggblock. The deep feature tensor produced by the mobile sub-model, \gls{xtensor} is $56 \times 56 \times 256$}
	\label{fig:expts:split:vgg}
\end{figure}

In a \gls{dfts} experiment, the deep feature tensor or tensors \gls{xtensor} originating from the mobile sub-model may be quantized uniformly to a chosen number of bits. The $\gls{h} \times \gls{w} \times \text{\#}\gls{c}$ tensors may be packetized with \gls{rpp} rows of deep feature values per packet. Zero padding may be applied to channels if the number of rows \gls{w} is not an integer multiple of the desired packet size. Deep feature tensors are packetized by assigning blocks of rows of each channel \gls{c} in a tensor to a packet $\gls{pkt}_i^{(\gls{c})}$. The packetized deep feature tensor data are then transmitted over a model of a communication channel to the cloud sub-model. This communication channel may be perfect (no loss) or lossy (Gilbert-Elliott or random loss channel). Since it was found in the literature that the Gilbert-Elliott channel model is an adequate model for packet transmission over communication networks, it was decided to focus experiments with Gilbert-Elliott channel realizations. Since most transmitted tensors investigated in this work have an integer multiple of eight rows of feature values per channel ($56 = 7 \times 8$ in the above \gls{vgg16} tensor), it is appropriate to use $\gls{rpp} = 8$ because it avoids the unnecessary zero-padding in \gls{dfts} packets.

Figure \ref{fig:expts:split:vgg:arch} shows the architecture of the mobile device sub-model and cloud sub-model which are created when a \gls{vgg16} is split at the output of \vggblock.

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/vgg16block3conv2mobile.png}
		\caption[VGG16 edge model example]{Edge sub-model}
	\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
	\centering
	\includegraphics[width=\textwidth]{Figures/vgg16block3conv2cloud.png}
	\caption[VGG16 cloud model]{Cloud sub-model}
\end{subfigure}
\caption[VGG16 edge and cloud models]{\gls{vgg16} split into a device model and a cloud model at the output of \vggblock.}
\label{fig:expts:split:vgg:arch}
\end{figure}

The edge sub-model produces a $56 \times 56 \times 256$ tensor \gls{xtensor} which is transmitted over a given channel to the cloud sub-model. A new input layer \verb|input_2| is added to the cloud sub-model to process the incoming tensor. The regular downstream architecture of a \gls{vgg16} is maintained as from the output of the new input layer.

Figure \ref{fig:expt:chain} illustrates the typical processing chain in a \gls{dfts} experiment. First, the edge sub-model takes the test image as input and produces an output tensor \gls{xtensor}. The deep feature tensor is quantized at the user-defined bit resolution. The quantized deep feature tensor is packetized and transmitted over a realization of a Gilbert-Elliott channel. When the deep feature packets are received at the cloud sub-model, they are inverse quantized. If requested, the corrupted deep feature tensors are repaired with a chosen tensor completion method. The resulting deep feature tensor is passed on to the cloud sub-model which does the remaining \gls{dnn} computations to complete the inference task.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.45]{Figures/processingchain.pdf}
	\caption{\gls{dfts} experiment processing chain}
	\label{fig:expt:chain}
\end{figure}

\section{Effect of quantization} \label{sec:expts:quant}
The effect of quantizing deep feature tensors is studied in this section. Quantization of deep feature tensor data before transmission over a communication channel may be seen as a means of reducing their footprint, which means shorter transmission times from the edge to the cloud and overall reduced inference latency. In this section of experiments on quantization, channel realization effects are removed by setting the communication channel to perfect. This means that no packet is lost in transmission. Since there is no random aspect to these simulations, they are single-shot experiments. The exact same prediction accuracy will be obtained every single time they are run. In a quantization experiment in \gls{dfts}, tensors are quantized at the user-specified resolution, transmitted over a perfect channel with no packet loss and inverse quantized before inference at the cloud.

Figure \ref{fig:expt:chainq} shows the processing chain for quantization experiments. The channel model and tensor completion module are disabled.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.45]{Figures/processingchainq.pdf}
	\caption{\gls{dfts} processing chain for quantization experiments}
	\label{fig:expt:chainq}
\end{figure}

\subsection{Results with VGG16} \label{subsec:expts:quant:vgg16}
Tensors originating from seven layers of a pre-trained \gls{vgg16} were tested over a range of quantization resolutions. The results over the entire test of 882 images are shown in Figure \ref{fig:expts:quant:vgg16}. It should be noted that these are simulated $n$-bit quantizations. The \gls{dfts} bit quantizer does not cast the tensor data variable from 32 bit floating points to another variable type, it rather simulates the effect of quantizing the actual deep feature tensor data. A batch size of 256 items was used in all experiments run on the \textit{Cedar} HPC cluster, unless stated otherwise. This means that the 882 images of the test set were split into four batches.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.75]{Figures/fig1quantvgg16.pdf}
	\caption[Effect of quantizing the edge sub-model tensor on Top-1 prediction accuracy with a VGG16]{Effect of quantizing the edge sub-model tensor on Top-1 prediction accuracy with a \gls{vgg16}.} \label{fig:expts:quant:vgg16}
\end{figure}
Figure \ref{fig:expts:quant:vgg16:arch} depicts a portion of the \gls{vgg16} architecture which contains the seven split layers investigated in the quantization experiments.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{Figures/vgg16quantannotated.png}
	\caption[VGG16 architecture]{Portion of \gls{vgg16} architecture containing the seven split layers investigated in the quantization experiments. At each split layer, the transmitted tensor's dimensions are given by the `Output' row.}
	\label{fig:expts:quant:vgg16:arch}
\end{figure}

We consider a layer which is further downstream, \verb|block5_pool| and a layer which is more upstream, such as \verb|block2_conv2| in Figure \ref{fig:expts:quant:vgg16:arch}: in Figure \ref{fig:expts:quant:vgg16}, the \verb|block5_pool| tensor is much less sensitive to poor quantization resolutions than the \verb|block2_conv2| tensor. The same applies to the other split layers: layers which are further downstream are more resilient to errors introduced by quantization. As can be seen in Figure \ref{fig:expts:quant:vgg16} and in Table \ref{table:quant:fig1:vgg16}, all split layers lead to quantized tensors producing identical classification accuracies for 8 bit and 10 bit quantization. At these quantization resolutions, the classification accuracy is identical to the case in which there was no quantization done at all (c.f. the dashed black line in Figure \ref{fig:expts:quant:vgg16}). This suggests that a 8-bit quantization leads to accurate representations of deep feature tensor data in \gls{vgg16} tensors.

\begin{table}[H]
	\caption[Effect of quantizing the edge sub-model tensor on Top-1 prediction accuracy with a VGG16]{Effect of quantizing the edge sub-model tensor on Top-1 prediction accuracy with a \gls{vgg16}.}\label{table:quant:fig1:vgg16}
	\centering 
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{|l|rrrrr|}
	\hline 
    \multicolumn{1}{|c|}{\textbf{Split layer}} &
    \multicolumn{5}{ c|}{$n$\textbf{-bit quantization}} \\ 
    {} 	&        2 Bit  &        4 Bit  &        6 Bit  &        8 Bit  &        10 Bit \\
    \hline
	block2\_conv2 &  0.137188 &  0.800454 &  0.837868 &  0.840136 &  0.840136 \\
	block3\_conv2 &  0.143991 &  0.832200 &  0.841270 &  0.840136 &  0.840136 \\
	block3\_pool  &  0.175737 &  0.832200 &  0.840136 &  0.840136 &  0.840136 \\
	block4\_conv2 &  0.399093 &  0.836735 &  0.841270 &  0.840136 &  0.840136 \\
	block4\_pool  &  0.575964 &  0.837868 &  0.840136 &  0.840136 &  0.840136 \\
	block5\_conv2 &  0.549887 &  0.836735 &  0.841270 &  0.840136 &  0.840136 \\
	block5\_pool  &  0.609977 &  0.833333 &  0.841270 &  0.840136 &  0.840136 \\
	\hline
\end{tabular}%
\end{adjustbox}
\end{table}

\subsection{Results with Resnet18} \label{subsec:expts:quant:resnet18}
Tensors originating from six layers of a pre-trained \gls{resnet18} were tested over the range of quantization resolutions $\{2,4,6,8,10\}$ bits. The results over the entire test of 882 images are shown in Figure \ref{fig:expts:quant:resnet18}. As with the corresponding experiment on \gls{vgg16}, these are simulated $n$-bit quantizations.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.75]{Figures/fig1quantresnet18.pdf}
	\caption[Effect of quantizing the edge sub-model tensor on Top-1 prediction accuracy with a Resnet18]{Effect of quantizing the edge sub-model tensor on Top-1 prediction accuracy with a \gls{resnet18}.} \label{fig:expts:quant:resnet18}
\end{figure}

Figure \ref{fig:expts:quant:resnet18:arch} indicates the layers at which the pre-trained \gls{resnet18} was split in the quantization experiments. As can be seen in Figure \ref{fig:expts:quant:resnet18:arch}, all chosen split layers except \verb|add_1| lead to two tensors being transmitted from the edge sub-model. However, for layers \verb|stage1_unit1_relu1| and \verb|add|, the two transmitted tensors are identical. Therefore, this means that a single tensor can be transmitted and then fed to both input layers of the cloud sub-model.

\gls{resnet18} deep feature tensors are very sensitive to rough quantization: prediction accuracies with 2-bit quantization are very close to $0\%$ in Figure \ref{fig:expts:quant:resnet18}. This contrasts with the results with \gls{vgg16} tensors in Subsection \ref{subsec:expts:quant:vgg16}, which are much more tolerant to quantization errors. The prediction accuracy in the best case scenario (no quantization) with \gls{resnet18} is $85.3741\%$, which is slightly higher (as expected) than the corresponding figure with \gls{vgg16} ($84.0136\%$). It is interesting to note that both addition layers (\verb|add| and \verb|add_1|) have the poorest classification accuracy for 6-bit quantized tensors. This means that deep feature tensors originating from layers which close a shortcut connection in the deep residual learning framework (c.f. Figure 2 in \cite{he2016deep}) poorly tolerate errors introduced by quantization. The next lowest Top-1 accuracy at 6 bits was experienced by \verb|stage1_unit1_relu1| tensors. Since this layer is the earliest split layer in the \gls{resnet18} architecture, it is more sensitive to quantization errors, as was the case in the \gls{vgg16} experiments.

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/resnet18quant1annotated.png}
		\caption{First set of split layers.} \label{fig:expts:quant:resnet18:arch:1}
	\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
	\centering
	\includegraphics[width=\textwidth]{Figures/resnet18quant2annotated.png}
	\caption{Second set of split layers.}
	\label{fig:expts:quant:resnet18:arch:2}
\end{subfigure}
\caption[ResNet18 architecture indicating split layers]{Portions of the \gls{resnet18} architecture containing the six split layers investigated in the quantization experiments.} \label{fig:expts:quant:resnet18:arch}
\end{figure}

Table \ref{table:quant:fig1:resnet18} presents the Top-1 accuracies at each chosen split layer in \gls{resnet18}. Some scenarios led to prediction accuracies which are slightly above the best case scenario accuracy of $85.3741\%$ (no quantization Top-1 accuracy). In such cases, it seems that the simulated quantization had a positive effect on the cloud's inference task. 

\begin{table}[H]
	\caption[Effect of quantizing the edge sub-model tensor on Top-1 prediction accuracy with a Resnet18]{Effect of quantizing the edge sub-model tensor on Top-1 prediction accuracy with a \gls{resnet18}.}\label{table:quant:fig1:resnet18}
	\centering 
	\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|l|rrrrr|}
	\hline 
	\multicolumn{1}{|c|}{\textbf{Split layer}} &
	\multicolumn{5}{c|}{$n$\textbf{-bit quantization}} \\ 
	{} 	&        2 Bit  &        4 Bit  &        6 Bit  &        8 Bit  &        10 Bit \\
	\hline
stage1\_unit1\_relu1 &  0.031746 &  0.695011 &  0.840136 &  0.853741 &  0.853741 \\
stage1\_unit1\_relu2 &  0.000000 &  0.792517 &  0.846939 &  0.853741 &  0.853741 \\
add                &  0.002268 &  0.596372 &  0.859410 &  0.851474 &  0.852608 \\
stage1\_unit2\_relu1 &  0.007937 &  0.778912 &  0.854875 &  0.853741 &  0.853741 \\
stage1\_unit2\_relu2 &  0.005669 &  0.787982 &  0.858277 &  0.853741 &  0.853741 \\
add\_1              &  0.000000 &  0.656463 &  0.851474 &  0.854875 &  0.854875 \\
	\hline
\end{tabular}%
\end{adjustbox}
\end{table}

This section has explored the effect of uniformly quantizing all tensors produced at a given split layer in a pre-trained \gls{resnet18}. It is also worth investigating how using different bit quantizations for each transmitted tensor impacts the cloud prediction accuracy. The following subsection discusses experiments on the \gls{resnet18} split layers which output two distinct tensors.

\subsubsection{Using different bit resolutions in quantizing each transmitted tensor}
This subsection investigates the effect of utilizing different bit resolutions on tensors produced at the layers which happen to produce two tensors to be transmitted in a \gls{ci} context. The capacity to quantize tensors at different bit resolutions is a new feature which was contributed to \gls{dfts} in the course of this project.

At the chosen split layer in a \gls{dfts} experiment, the \gls{resnet18} architecture is split into an device sub-model and a cloud sub-model at the layer's output. Due to the skip connections present in a residual \gls{dnn}, there may be an additional tensor to be transmitted. As shown previously in the architecture diagrams of Figure \ref{fig:expts:quant:resnet18:arch}, due to skip connections, two tensors are transmitted at the output of \verb|stage1_unit1_relu1|, \verb|stage1_unit1_relu2|, \verb|add|, \verb|stage1_unit2_relu1| and \verb|stage1_unit2_relu2|. 

In the case of \verb|stage1_unit1_relu2|, \verb|stage1_unit2_relu1| and \verb|stage1_unit2_relu2|, the two transmitted tensors are distinct from each other because they originate from different layers in the device sub-model architecture. It is worthwhile to investigate the end effect of quantizing these tensors at different resolutions.

\textbf{Split layer:} \soneunitonerelutwo 

Figure \ref{fig:expts:quant:s1u1r2} shows the architectures for the edge and the cloud sub-models when a \gls{resnet18} is split at the output of \soneunitonerelutwo.

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.34\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/stage1unit1relu2mobilemodelannotated.png}
		\caption{Device model}
	\end{subfigure}
	\hfill 
	\begin{subfigure}{0.64\textwidth}
		\centering
		\includegraphics[width= \textwidth]{Figures/stage1unit1relu2cloudmodelannotated.png}
		\caption{Cloud model}
	\end{subfigure}
\caption[ResNet18 split: two tensors transmitted from the edge to the cloud]{Architectures for the edge sub-model and cloud sub-model when a \gls{resnet18} is split at \soneunitonerelutwo.} \label{fig:expts:quant:s1u1r2}
\end{figure}

Tensor 0 is produced by \soneunitonerelutwo~is transmitted over the channel and is fed to the new input layer \verb|input_3|. The second tensor, tensor 1, from \verb|stage1_unit1_relu1|, is fed to the skip connection to input layer \verb|input_4|. The latter precedes layer \soneunitonesc~which belonged to a residual branch of \gls{resnet18}. Layers involved in the relevant skip connection are made to reside in the cloud sub-model after the \gls{dnn} is split. Given their different origins, tensors 0 and 1 are distinct from each other.

%\begin{figure}[H]
%	\centering
%	\includegraphics[scale=0.5]{Figures/stage1unit1relu1annotated.png}
%	\caption[ResNet18 architecture split at the output of stage 1 unit 1 relu 1]{\gls{resnet18} architecture split at the output of \soneunitonereluone. There are two tensors transmitted: tensor 0 to \zpttwidtwo~and tensor 1 to \soneunitonesc.}
%\end{figure}
%Layers above and including \soneunitonereluone~ form the mobile sub-model. The cloud sub-model is augmented with two layers, \verb|input_3| and \verb|input_4| to serve as 
%input layer to the tensors received from the device, as can be seen in Figure 
%


%\begin{figure}[H]
%	\centering 
%	\includegraphics[scale=0.87]{Figures/fig1quantresnet18stage1unit1relu1.pdf}
%	\caption[ResNet18 stage1 unit1 relu1]{\gls{resnet18} stage1 unit1 relu1}
%\end{figure}


%\begin{figure}[H]
%	\centering 
%	\includegraphics[scale=0.87]{Figures/fig1quantresnet18add.pdf}
%	\caption[ResNet18 add1]{\gls{resnet18} add1}
%\end{figure}

Figure \ref{fig:expt:quant:s1u1r2:pred} shows the Top-1 cloud prediction accuracy when the two input tensors are quantized at different bit resolutions. Tensor 0, from the main branch of \gls{resnet18}, is fed to \verb|input_3| and tensor 1, from the skip connection, is fed to \verb|input_4|.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=0.85]{Figures/fig1quantresnet18stage1unit1relu2.pdf}
	\caption[Prediction accuracy for stage 1 unit 1 relu 2 quantization experiment]{Cloud prediction accuracy when the two input tensors are quantized at different bit resolutions for \soneunitonerelutwo.}
	\label{fig:expt:quant:s1u1r2:pred}
\end{figure}

When tensor 0 is coarsely quantized at 2 bits, the cloud prediction accuracy remains very low (much less than $1\%$) regardless of the quantization resolution employed for tensor 1. This suggests that tensor 1 is more `significant' than tensor 0 at layer \soneunitonerelutwo. This is further evidenced by the fact that it is possible to reach a Top-1 prediction accuracy of $85\%$ by using an 8-bit quantized tensor 1 and a 4-bit quantized tensor 0 but it is not possible when the quantization resolutions are swapped around.

\textbf{Split layer:} \soneunittworeluone

When a \gls{resnet18} is split at the output of \soneunittworeluone, two tensors are produced. Tensor 0, from the output of \soneunittworeluone, is fed to the input layer \verb|input_3| in the cloud model. Tensor 1, from the output of \verb|add| (c.f. Figure \ref{fig:expts:quant:resnet18:arch:2}), is fed to the input layer \verb|input_4| in the cloud model. Figure \ref{fig:expt:quant:s1u2r1} shows the Top-1 cloud prediction accuracy when the two input tensors are quantized at different bit resolutions.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=0.85]{Figures/fig1quantresnet18stage1unit2relu1.pdf}
	\caption[Prediction accuracy for stage 1 unit 2 relu 1 quantization experiment]{Cloud prediction accuracy when the two input tensors are quantized at different resolutions for \soneunittworeluone}
	\label{fig:expt:quant:s1u2r1}
\end{figure}

For \soneunittworeluone, there is less disparity in significance for the cloud prediction performance. The cloud's performance is more tolerant of poor quantization of tensor 0 (fed into \verb|input_3|) than of tensor 1: it is possible to reach 25\% accuracy with a 2-bit quantized tensor 0. However, if tensor 1 is coarsely quantized at 2 bits, the classification accuracy is poor regardless of the quantization level of tensor 0.

\textbf{Split layer:} \soneunittworelutwo

Two tensors are transmitted: tensor 0, from the output of \soneunittworelutwo, is fed to the input layer \verb|input_3| in the cloud model. Tensor 1, from the output of \verb|add| (c.f. Figure \ref{fig:expts:quant:resnet18:arch:2}), is fed to the input layer \verb|input_4| in the cloud model. Figure \ref{fig:expt:quant:s1u2r2:pred} shows the Top-1 cloud prediction accuracy when the two input tensors are quantized at different bit resolutions.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=0.85]{Figures/fig1quantresnet18stage1unit2relu2.pdf}
	\caption[Prediction accuracy for stage1 unit2 relu2 quantization experiment]{Cloud prediction accuracy when the two input tensors are quantized at different resolutions for \soneunittworelutwo}
	\label{fig:expt:quant:s1u2r2:pred}
\end{figure}

Similarly to the experimental results with \soneunittworeluone, the cloud's prediction performance is more tolerant of poor quantization of tensor 0 than of tensor 1 when the \gls{resnet18} is split at \soneunittworelutwo. In spite of the coarse 2-bit quantized tensor 0, the cloud can reach a prediction accuracy of 25 to 45 percent when a larger number of bits (4 to 10) are used to quantize tensor 1.  However, if tensor 1 is coarsely quantized at 2 bits, the classification accuracy is poor regardless of the quantization level of tensor 0. This again indicates that tensor 1 has a greater significance than tensor 0 in the cloud's prediction performance. 

For the three layers considered in this subsection, the cloud model was much less tolerant of quantization errors in tensor 1 than in tensor 0. In these three cases, tensor 0 belonged to the main branch of \gls{resnet18} and tensor 1 was associated with the skip connection.

%\begin{table}[H]
%	\caption[Top-1 Accuracy in predictions when using different bit resolutions on two tensors]{Top-1 Accuracy in predictions with different bit resolutions with a \gls{resnet18} split at the output of \stageoneunitonerelutwo}\label{table:quant:s1u1r2}
%	\centering 
%	\begin{adjustbox}{max width=\textwidth}
%\begin{tabular}{llrr}
%	\toprule
%	\multicolumn{2}{l}{} &
%	\multicolumn{2}{c}{\inputfour} \\ 
%	{} &    {} &     2 Bit &         8 Bit \\
%	\midrule
%	\multirow{2}{*}{\inputthree} &
%	2 Bit &  $0.2268\%$ &  $62.0181\%$ \\
%{} &	8 Bit &  $1.0204\%$ &  $85.3741\%$ \\
%	\bottomrule
%\end{tabular}%
%\end{adjustbox}
%\end{table}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[scale=0.5]{Figures/fullmodels1u1r2.png}
%	\caption[Resnet18 split at the output of stage 1 unit 1 relu 2]{A \gls{resnet18} showing the split layer \stageoneunitonerelutwo.}
%\end{figure}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[scale=0.5]{Figures/cloudmodels1u1r2.png}
%	\caption[Cloud sub-model architecture with Resnet18]{Cloud sub-model architecture. Note that \inputthree~ is the input layer to \zptwodthree~ and \inputfour~ is the input layer to \soneunitonesc.}
%\end{figure}

%\begin{table}[H]
%	\caption[Top-1 Accuracy in predictions when using different bit resolutions on two tensors]{Top-1 Accuracy in predictions with different bit resolutions with a \gls{resnet18} split at the output of \stageoneunittworelutwo}\label{table:quant:s1u2r2}
%	\centering 
%	\begin{adjustbox}{max width=\textwidth}
%		\begin{tabular}{llrr}
%			\toprule
%			\multicolumn{2}{l}{} &
%			\multicolumn{2}{c}{\inputfour} \\ 
%			{} &    {} &     2 Bit &         8 Bit \\
%			\midrule
%			\multirow{2}{*}{\inputthree} &
%			2 Bit &  $2.9478\%$ &  $53.2880\%$ \\
%			{} &	8 Bit &  $6.3492\%$ &  $85.3741\%$ \\
%			\bottomrule
%		\end{tabular}%
%	\end{adjustbox}
%\end{table}\textbf{}

\section{Effect of channel parameters} \label{sec:expts:channel}
This section explores the effect of varying the imperfect communication channel's characteristics on the cloud prediction performance. A set of experiments on a \gls{vgg16} are discussed in Subsection \ref{sec:expts:channel:vgg16}. Subsection \ref{sec:expts:channel:resnet} investigates the effect of changing channel parameters when a \gls{resnet18} is split at \verb|add_1|, \verb|stage1_unit1_relu2| and \verb|stage1_unit2_relu2|. All tensors were quantized to 8 bits in these sets of experiments since 8 bits are usually sufficient to reach the highest prediction accuracy possible. Tensors were packetized with eight rows of deep feature values per packet ($\gls{rpp} = 8$) in all experiments because all tensors had a height \gls{h} which was divisible by eight. No error concealment or tensor completion techniques were applied to the damaged tensors \gls{xda} to ensure that the actual impact of the unreliable channels would be apparent.

\subsection{Results with VGG16} \label{sec:expts:channel:vgg16}
Tensors from an edge model consisting of \gls{vgg16} layers up to \verb|block3_conv2| were transmitted over Gilbert-Elliott channels given by a loss probability of $\gls{pb} \in \{1\%,10\%,20\%,30\%\}$ and a burst length $\gls{bl} \in \{1,2,3,4,5,6,7\}$ packets. Since channel realizations are random, it was necessary to simulate a number of Monte Carlo runs to obtain results which were representative of the channel's characteristics. \verb|block3_conv2| deep feature tensors are $56 \times 56 \times 256$ and eight rows of values in a channel constitute a packet ($\gls{rpp}=8$), so a burst of seven packets means that an entire $56 \times 56$ channel is knocked out of a tensor \gls{xtensor}.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=0.75]{Figures/fig2vgg16block3conv2std.pdf}
	\caption[Prediction accuracy with VGG16 tensors transmitted over Gilbert-Elliott channels]{Top-1 accuracy averaged over 100 Monte Carlo runs of \gls{vgg16} tensors transmitted over Gilbert-Elliott channels. Error bars indicate the standard deviation in classification accuracy for each burst length.} \label{fig:expts:channel:vgg16}
\end{figure}

The cloud prediction accuracy deteriorates as the probability of a burst loss \gls{pb} increases from $1\%$ to $30\%$. As \gls{pb} increases, a larger proportion of the deep feature tensor packets are lost. This means that there are fewer packets \gls{pkt} for the cloud sub-model to exploit in order to complete the inference task. When the average burst length \gls{bl} increases, this means that, on average, longer contiguous macro-blocks of deep feature tensor data are lost. The cloud prediction performance on quantized \gls{vgg16} tensor packets appears to be mostly independent of the average burst length. For the given Gilbert-Elliott channel parameters, the cloud prediction accuracy drops by less than $10\%$ in the worst cases (with $\gls{pb}=30\%$). This is because a Rectified Linear Unit (ReLU) activation function is applied to the output of convolutional layers, such as \verb|block3_conv2|, in a \gls{vgg16}, thereby ensuring that \gls{vgg16} tensor packets always contain a large number of zeros. Since \gls{dfts} represents lost packets in a received corrupted tensor \gls{xda} as contiguous rows of zeros (a lost packet $\gls{pkt}_\text{lost}$ is a $\gls{rpp} \times \gls{w}$ matrix of zeros), damaged \gls{vgg16}
tensors are relatively similar to their non-damaged counterparts. Therefore, there is minimal benefit to be gained in `completing' corrupted \gls{vgg16} tensors with our tensor completion methods. Hence, our tensor completion experiments will not make use of corrupted \gls{vgg16} tensors.

\subsection{Results with ResNet18} \label{sec:expts:channel:resnet}
This subsection investigates the effect of Gilbert-Elliott channel parameters on quantized \gls{resnet18} tensors. The \gls{dnn} is split at a layer with a single tensor, \verb|add_1|, and two layers which involve the transmission of two deep feature tensors, \verb|stage1_unit1_relu2| and \verb|stage1_unit2_relu2|. A hundred Monte Carlo runs were performed for each experiment investigating each possible pair of Gilbert-Elliott channel parameters (loss probability $\gls{pb} \in \{1\%,10\%,20\%,30\%\}$ and average burst length $\gls{bl} \in \{1,2,3,4,5,6,7\}$ packets).

\textbf{Split layer:} \addone

In this set of experiments, deep feature tensors from a device sub-model for each test image in the 882 images dataset were quantized and were transmitted over a Gilbert-Elliott channel given by a pair of parameters $(\gls{pb},\gls{bl})$ for each Monte Carlo run. The received corrupted deep feature tensors \gls{xda} were inverse quantized and passed to the cloud sub-model. No error concealment or tensor completion were applied to the damaged tensors.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=0.75]{Figures/fig2resnet18add1std.pdf}
	\caption[Prediction accuracy with ResNet18 add 1 tensors transmitted over Gilbert-Elliott channels]{Top-1 accuracy averaged over 100 Monte Carlo runs of \gls{resnet18} \addone~tensors transmitted over Gilbert-Elliott channels. Error bars indicate the standard deviation in classification accuracy for each burst length.} \label{fig:expts:channel:add1}
\end{figure}

Figure \ref{fig:expts:channel:add1} shows that the Top-1 accuracy deteriorates drastically as the probability of a burst loss \gls{pb} increases from $1\%$ to $30\%$. As \gls{pb} increases, more deep feature packets \gls{pkt} are lost and are therefore not available for the cloud model to exploit in completing the inference task. The cloud prediction performance is much more adversely affected by packet loss for the \gls{resnet18} \verb|add_1| case than the \gls{vgg16} \verb|block3_conv2| case. In the worst case channel configurations (loss probability $\gls{pb}=30\%$), the cloud prediction accuracy remains above $75\%$ for \gls{vgg16} regardless of burst length (c.f. Figure \ref{fig:expts:channel:vgg16}.) The corresponding cloud prediction accuracy remains less than $40\%$ for all burst lengths for \gls{resnet18}. This can be explained by the fact that \gls{resnet18} performs a batch normalization step after the ReLU activation functions. This re-centers the data distribution away from the zeros produced by the rectified linear unit activations. Therefore, \gls{resnet18} tensors typically contain fewer zeros than \gls{vgg16} tensors, which makes them inherently more sensitive to packet loss.

\textbf{Split layer:} \soneunitonerelutwo 

Similarly to the set of experiments run for \verb|add_1| tensors, deep feature packets from tensors 0 and 1 from an edge sub-model are transmitted over a Gilbert-Elliott channel given by a pair of parameters $(\gls{pb},\gls{bl})$ for each Monte Carlo run. Both tensors were quantized to 8 bits and are inverse quantized at the cloud.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=0.75]{Figures/fig2resnet18stage1unit1relu2std.pdf}
	\caption[Prediction accuracy with ResNet18 stage 1 unit 1 relu 2 tensors transmitted over Gilbert-Elliott channels]{Top-1 accuracy averaged over 100 Monte Carlo runs of \gls{resnet18} \soneunitonerelutwo~tensors transmitted over Gilbert-Elliott channels. Error bars indicate the standard deviation in classification accuracy for each burst length.} \label{fig:expts:channel:s1u1r2}
\end{figure}

Figure \ref{fig:expts:channel:s1u1r2} shows that packet loss and quantization errors have a much more pronounced effect on the cloud prediction accuracy when the deep neural network is split at the output of \soneunitonerelutwo~than when the \gls{resnet18} is split at the output of \addone~(c.f. Figure \ref{fig:expts:channel:add1}). 

In \soneunitonerelutwo~experiments, the highest prediction accuracy was approximately $60\%$ which is more than $10\%$ lower than the highest prediction accuracy achieved on \addone~tensors. Furthermore, the worst prediction accuracy with \addone~tensors exceeded $30\%$ whereas it is less than $10\%$ with \soneunitonerelutwo~tensors. 

Lower prediction accuracies are achieved when Gilbert-Elliott channels with smaller average burst lengths are realized. For instance, for the $\gls{pb}=30\%$ curve, the average prediction accuracy improves as the average burst length $\gls{bl}$ increases from one to seven packets. Given a fixed total number of lost packets according to a given value of loss probability \gls{pb}, the lost packets are spread out more across the deep feature tensor channels when the average burst length is short. When \gls{bl} is long, say 6 packets long, the losses are concentrated in fewer channels of the deep feature tensor. The cloud model is more tolerant of concentrated losses when the \gls{resnet18} is split at the output of \soneunitonerelutwo.

\textbf{Split layer:} \soneunittworelutwo

As before, the deep feature packets from tensor 0 and tensor 1 from an edge sub-model are transmitted over a Gilbert-Elliott channel given by a pair of parameters $(\gls{pb},\gls{bl})$ for each Monte Carlo run. Both tensors were quantized to 8 bits and are inverse quantized at the cloud.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=0.75]{Figures/fig2resnet18stage1unit2relu2std.pdf}
	\caption[Prediction accuracy with ResNet18 stage 1 unit 2 relu 2 tensors transmitted over Gilbert-Elliott channels]{Top-1 accuracy averaged over 100 Monte Carlo runs of \gls{resnet18} \soneunittworelutwo~tensors transmitted over Gilbert-Elliott channels. Error bars indicate the standard deviation in classification accuracy for each burst length.} \label{fig:expts:channel:s1u2r2}
\end{figure}

Figure \ref{fig:expts:channel:s1u2r2}  shows that, with \soneunittworelutwo~tensors transmitted over Gilbert-Elliott channels, packet loss and quantization errors have a suppressed effect on the cloud prediction performance as compared to \soneunitonerelutwo~tensors (c.f. Figure \ref{fig:expts:channel:s1u1r2}). All constant loss probability graphs in Figure \ref{fig:expts:channel:s1u2r2} lie above the corresponding graphs in Figure \ref{fig:expts:channel:s1u1r2}. In the \gls{resnet18} architecture (c.f. Figure \ref{fig:expts:quant:resnet18:arch:2}), \soneunittworelutwo~is further downstream than \soneunitonerelutwo~ which explains this observation. Deeper layers in a \gls{dnn} are generally more resilient to missing or damaged deep feature values.

\section{Repairing damaged tensors} \label{sec:expts:repair}
This section discusses experiments developed to illustrate how tensor completion methods used in this work repair damaged tensors. Two interesting examples, an ostrich image and a starfish image, are used in this investigation.

The ostrich image is input to the device sub-model. The resulting tensor is quantized to 8-bits and then packetized. It is transmitted over a Gilbert-Elliott channel with parameters $(\gls{pb},\gls{bl}) = (0.3,4)$. Channel 1 of the corrupted tensor \gls{xda} is selected because 5 out of the seven packets of this channel were missing. Figure \ref{fig:expt:repair:ostrich} shows the original channel, the damaged channel and the repaired versions with the proposed method \gls{caltec} and the two general tensor completion methods.


\begin{figure}[H]
\centering
\begin{subfigure}{.32\linewidth}
	\centering
	\includegraphics[width = \linewidth]{Figures/originalbatch0item0tensor0channel1.jpg}
	\caption{Original channel.}
\end{subfigure}%
\hfill
%\hspace{1em}% Space between image A and B
\begin{subfigure}{.32\textwidth}
	\centering
	\includegraphics[width = \textwidth]{Figures/damagedbatch0item0tensor0channel1.jpg}
	\caption{Damaged channel.}
\end{subfigure}
\hfill 
%
%\hspace{1em}% Space between image B and C
\begin{subfigure}{.32\textwidth}
	\centering
	\includegraphics[width = \textwidth]{Figures/calteclumibatch0item0tensor0channel1scaled.jpg}
	\caption{CALTeC output.}
\end{subfigure}
%\vspace{2.5em}

\begin{subfigure}{.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{Figures/halrtc50batch0item0tensor0channel1.jpg}
	\caption{HaLRTC 50 iterations.}
\end{subfigure}
\hfill 
%\hspace{1em}
\begin{subfigure}{.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{Figures/halrtc100batch0item0tensor0channel1.jpg}
	\caption{HaLRTC 100 iterations.}
\end{subfigure}
\hfill
%\hspace{1em}
\begin{subfigure}{.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{Figures/halrtc200batch0item0tensor0channel1.jpg}
	\caption{HaLRTC 200 iterations.}
\end{subfigure}

\begin{subfigure}{.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{Figures/silrtc50batch0item0tensor0channel1.jpg}
	\caption{SiLRTC 50 iterations.}
\end{subfigure}
\hfill 
%\hspace{1em}
\begin{subfigure}{.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{Figures/silrtc100batch0item0tensor0channel1.jpg}
	\caption{SiLRTC 100 iterations.}
\end{subfigure}
\hfill
%\hspace{1em}
\begin{subfigure}{.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{Figures/silrtc200batch0item0tensor0channel1.jpg}
	\caption{SiLRTC 200}
\end{subfigure}

\caption[Channel 1 visualization in a damaged ostrich image ResNet18 tensor]{Channel 1 in a damaged \textit{ostrich} \gls{resnet18} \addone~ tensor \& repaired with \gls{caltec}, \gls{halrtc} and \gls{silrtc}. (Note that these images were scaled up from their original $56 \times 56$ size by a factor of 5 using bicubic interpolation.)}
	\label{fig:expt:repair:ostrich}
\end{figure}

The repaired channel after 50, 100 and 200 iterations of \gls{silrtc} and \gls{halrtc} are shown. As the number of iterations of these two tensor completion methods increases, the `completed' tensor's rank is reduced further. The Frobenius norm of the difference between the original channel and the `completed' channel decreases. Since \gls{caltec} is not an iterative procedure, a single pass through with \gls{caltec} also decreases the Frobenius norm of the difference between the original channel and the `completed' channel. More importantly, the proposed method manages to recover a channel which appears much more similar visually to the original channel than the other two methods. It is noteworthy that \gls{caltec} uses only two existing packets to recover the other five packets which constitute channel 1.

Figure \ref{fig:expt:repair:starfish} visualizes different versions of channel 1 for the starfish image. Four out of seven packets are missing in the damaged channel. There were three burst of loss since two lost packets were contiguous. The `completed' tensor after 50, 100 and 200 iterations of \gls{silrtc} and \gls{halrtc} are shown. As with the ostrich image example, \gls{silrtc} and \gls{halrtc} have not recovered the shape of the starfish after 200 iterations. In contrast, a single pass through with \gls{caltec} has recovered most of the general shape of the starfish.

\gls{silrtc} and \gls{halrtc} are expected to produce better results as they are iterated more over the damaged tensor \gls{xda}. It is therefore worthwhile to investigate how many iterations of these two methods are necessary to adequately repair a damaged tensor.

\begin{figure}[H]
	\centering
	\begin{subfigure}{.32\linewidth}
		\centering
		\includegraphics[width = \linewidth]{Figures/originalbatch0item1tensor0channel17scaled.jpg}
		\caption{Original channel.}
	\end{subfigure}%
	\hfill
	%\hspace{1em}% Space between image A and B
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width = \textwidth]{Figures/damagedbatch0item1tensor0channel17scaled.jpg}
		\caption{Damaged channel.}
	\end{subfigure}
	\hfill 
	%
	%\hspace{1em}% Space between image B and C
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width = \textwidth]{Figures/calteclumibatch0item1tensor0channel17scaled.jpg}
		\caption{CALTeC output.}
	\end{subfigure}
	%\vspace{2.5em}
	
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/halrtc50batch0item1tensor0channel17scaled.jpg}
		\caption{HaLRTC 50 iterations.}
	\end{subfigure}
	\hfill 
	%\hspace{1em}
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/halrtc100batch0item1tensor0channel17scaled.jpg}
		\caption{HaLRTC 100 iterations.}
	\end{subfigure}
	\hfill
	%\hspace{1em}
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/halrtc200batch0item1tensor0channel17scaled.jpg}
		\caption{HaLRTC 200 iterations.}
	\end{subfigure}
	
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/silrtc50batch0item1tensor0channel17scaled.jpg}
		\caption{SiLRTC 50 iterations.}
	\end{subfigure}
	\hfill 
	%\hspace{1em}
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/silrtc100batch0item1tensor0channel17scaled.jpg}
		\caption{SiLRTC 100 iterations.}
	\end{subfigure}
	\hfill
	%\hspace{1em}
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/silrtc200batch0item1tensor0channel17scaled.jpg}
		\caption{SiLRTC 200 iterations.}
	\end{subfigure}
	
	\caption[Channel 1 visualization in a damaged starfish image ResNet18 tensor]{Channel 1 in a damaged \textit{starfish} \gls{resnet18} \addone~ tensor \& repaired with \gls{caltec}, \gls{halrtc} and \gls{silrtc}. (Note that these images were scaled up from their original $56 \times 56$ size by a factor of 5 using bicubic interpolation.)}
	\label{fig:expt:repair:starfish}
\end{figure}


\section{Finding the minimum suitable iteration count for tensor completion} \label{sec:expts:iter}

The previous section pointed out that \gls{silrtc} and \gls{halrtc} repaired tensors are improved through several iterations. This section reports on experiments performed to find the adequate number of tensor completion iterations to run to repair a damaged tensor.

In this set of experiments, the Gilbert-Elliot channel model chosen was the one which gave the poorest prediction accuracy in Section \ref{sec:expts:channel:resnet}, $(\gls{pb},\gls{bl}) = (0.3,1)$ (c.f. Figure \ref{fig:expts:channel:add1}). Two hundred iterations of \gls{silrtc} and \gls{halrtc} were run on corrupted \addone~ tensors generated from the 882 images of the test set.

The Frobenius norm of the difference between a `completed' tensor \gls{xtensor} from one iteration $K$ to the following iteration $K+1$ is given by:

\begin{equation}
	\lVert \hat{\mathcal{A}} \rVert_F = \lVert \gls{xtc}^{K+1} - \gls{xtc}^{K} \rVert_F = \sqrt{\sum_{i,j,l} | \gls{xtc}_{i,j,l}^{K+1} - \gls{xtc}_{i,j,l}^{K} |^2}
\end{equation}

The Frobenius norm is calculated from the sum of the squares of the element-wise difference between two tensors. Figure \ref{fig:expts:maxiters} shows the Frobenius norm of the difference tensor in log scale and linear scale.

\begin{figure}[H]
	\centering
	\begin{subfigure}{.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/maxiterspostprocessingadd1errorlog.pdf}
		\caption{Log scale.}
	\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\textwidth]{Figures/maxiterspostprocessingadd1error.pdf}
	\caption{Linear scale.}
\end{subfigure}
	\caption[Difference between completed tensors over successive iterations of tensor completion in ResNet18]{Frobenius norm of the difference between completed tensors over successive iterations of tensor completion on \gls{resnet18} \addone~ tensors.} \label{fig:expts:maxiters}
\end{figure}

The linear scale plot suggests that \gls{silrtc} and \gls{halrtc} have converged before reaching 25 iterations since the relative difference seems to have reached a minimum (zero or approximately zero). The logarithmic scale plot shows that the \gls{halrtc} Frobenius norm is still decreasing after 200 iterations. The \gls{silrtc} Frobenius norm is still decreasing (at a much smaller rate) after 200 iterations. This does not necessarily mean that a better tensor recovery performance will be obtained by iterating further because the magnitude of the tensor updates (the y-axis scale in Figure \ref{fig:expts:maxiters}) is arguably negligibly small.

%Over the test set consisting of 882 images, \gls{caltec} took on average $300~\mathrm{s}$ per Monte Carlo run. When presented with the same packet loss realizations corresponding to a Gilbert-Elliott channel given by $(\gls{pb}=0.3,\gls{bl}=1)$, \gls{silrtc} and \gls{halrtc} took on average $31000~\mathrm{s}$ and $32000~\mathrm{s}$ respectively. These timing records were obtained under identical simulation environments on Cedar.

Figure \ref{fig:expts:maxiters:pred} shows the prediction accuracy on \addone~tensors with 50, 100, 150 and 200 iterations of tensor completion.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{Figures/maxiterspostprocessingadd1predacc1.pdf}
	\caption[Prediction accuracy on ResNet18 tensors over a range of iterations of tensor completion.]{Prediction accuracy on \gls{resnet18} \addone~ tensors over $\{50,100,150,200\}$ iterations of \gls{silrtc} and \gls{halrtc}.} \label{fig:expts:maxiters:pred}
\end{figure}

The upper bound on performance is set by the scenario in which the \gls{resnet18} tensors are not quantized and are transmitted over a lossless channel. The lower bound on performance is set by the scenario in which the \gls{resnet18} tensors are quantized and transmitted over a Gilbert-Elliott channel given by ($\gls{pb}=30\%,\gls{bl}=1$). As the number of iterations increases, the \gls{silrtc} prediction accuracy improves significantly but it is still more than 10\% lower than the \gls{halrtc} prediction accuracy. The \gls{halrtc} prediction accuracy is already high after 50 iterations and does not improve much beyond 50 iterations.

\section{Experiments under default settings} \label{sec:expts:default}

This section reports on Monte Carlo experiments in which deep feature tensors from a \gls{resnet18} were transmitted over Gilbert-Elliott channels. Since this section continues with channel realization experiments in a similar vein to those discussed in Section \ref{sec:expts:channel}, the same configurations were used. Deep feature tensors were quantized to 8 bits and packetized with eight rows of values per packet $(\gls{rpp} = 8)$. Gilbert-Elliott channels were realized with the same parameters - loss probabilities $\gls{pb}$ of 1\%, 10\%, 20\% and 30\% and average burst lengths $\gls{bl}$ between 1 and 7 packets long. A batch size of 256 images was used in all experiments run on the \textit{Cedar} compute cluster. 

To ensure that processing time comparisons are fair, the exact same hardware configuration was requested in all experiments run on \textit{Cedar}. Tensor completion methods were run for 50 iterations in all experiments. The Singular Value Decomposition code in our implementation of \gls{silrtc} and \gls{halrtc} was in pure Numpy. It did not exploit GPU resources to accelerate the tensor completion simulations. Hence, \gls{silrtc} and \gls{halrtc} experiments were challenging in terms of requesting processing time on \textit{Cedar}. The time constraints led to the number of Monte Carlo runs being kept to 10 for each pair of Gilbert-Elliott parameters $(\gls{pb},\gls{bl})$. 

Furthermore, the tensor completion methods were all presented with the exact same packet loss pattern in a given Monte Carlo experiment. This ensures that a comparison on equal footing can be made. When generating realizations of a Gilbert-Elliott channel for a given Monte Carlo run, the \gls{dfts} script was made to store the packet loss pattern for each batch of test data as a Numpy \verb|.npy| file. This Numpy file was then used to re-create the packet loss pattern in the related \gls{caltec}, \gls{silrtc} and \gls{halrtc} Monte Carlo experiment. All tensor completion methods being presented with the same data compensates for the fact that the number of Monte Carlo runs was relatively small (10 for each pair of loss probability and burst length).

\textbf{Example: loss probability of 30\%}

We consider the experiment with the highest probability of burst loss. the first \gls{dfts} script generates 10 Monte Carlo runs worth of packet loss patterns and computes the cloud Top-1 prediction accuracy in the best-case scenario (no quantization of deep feature tensor values and no Gilbert-Elliott channel - referred to as NL in this section) and the worst-case scenario (Gilbert-Elliott channel with no tensor completion -referred to as NC in this section). These two cases set the performance bounds for our experiments. The remaining \gls{dfts} scripts run one of the tensor completion methods on quantized deep feature tensor packets after applying the relevant packet loss pattern to them and inverse quantizing the packets.

Figure \ref{fig:expts:default:30} shows the cloud prediction accuracy on \addone~tensors which have been transmitted over Gilbert-Elliott channels with $\gls{pb} = 30\%$ and average burst lengths ranging from one packet to seven packets long. The best case scenario (NL) corresponds to \gls{resnet18}'s best performance of 85.3741\%. The worst-case scenario leads to a cloud prediction accuracy of less than 40\%. As was found in Section \ref{sec:expts:channel}, the cloud's performance on \addone~tensors is not resilient to quantization and packet loss.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=1]{Figures/lp03caltec.pdf}
	\caption[Monte Carlo experiment results with loss probability 0.3 on ResNet18 tensors]{Prediction accuracy on \addone~tensors transmitted through Gilbert-Elliott channels with $\gls{pb} = 30\%$.}
	\label{fig:expts:default:30}
\end{figure}

The proposed method, \gls{caltec}, achieves the best Top-1 accuracy over all burst lengths. \gls{halrtc} provides the second best performance and significantly out-performs \gls{silrtc}. At this high loss probability, no tensor completion method can reach within 10\% of the no quantization and no packet loss case.

To compare the relative strengths of our tensor completion methods, we consider Figure \ref{fig:expts:default:30:time} which plots the tensor completion time against the Top-1 prediction accuracy for each average burst length value over the same set of experiments.

\begin{figure}[H]
	\centering
	\includegraphics[scale=1]{Figures/lp03time.pdf}
	\caption[Prediction accuracy versus tensor completion time with Gilbert-Elliott channels with loss probability 0.3 on ResNet18 tensors]{Prediction accuracy versus tensor completion time on \gls{resnet18} \addone~tensors transmitted over Gilbert-Elliott channels with $\gls{pb} = 30\%$.}
	\label{fig:expts:default:30:time}
\end{figure}

With an execution time of less than 15 minutes, \gls{caltec} is the fastest method. \gls{halrtc} is the slowest method over all seven average burst lengths investigated. Both \gls{silrtc} and \gls{halrtc} take more than 2 hours for a single Monte Carlo run through the test set. The execution times quoted in this figure do not include any time spent on file reading or writing or computations with the device or cloud sub-models. Only the time spent on tensor completion was factored in.

%
%\begin{figure}[H]
%	\centering 
%	\includegraphics[scale=1]{Figures/lp02caltec.pdf}
%	\caption[Monte Carlo experiment results with loss probability 0.2]{Prediction accuracy on repaired tensors transmitted through Gilbert-Elliott channels with $\gls{pb} = 20\%$.}
%\end{figure}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[scale=1]{Figures/lp02time.pdf}
%	\caption[Prediction accuracy versus tensor completion time with Gilbert-Elliott channels with loss probability 0.2]{Prediction accuracy versus tensor completion time on tensors transmitted through Gilbert-Elliott channels with $\gls{pb} = 20\%$.}
%\end{figure}


%
%\begin{table}[H]
%	\caption[ResNet18 Monte Carlo results under default settings]{\gls{resnet18} \addone~Monte Carlo results under default settings. \textbf{NL} refers to the scenario with no packet loss and no quantization. \textbf{NC} refers to the scenario with the Gilbert-Elliott channel and quantization but with no tensor completion. \textbf{TC} refers to the scenario with the Gilbert-Elliott channel and quantization followed by tensor completion. $\mu$ and $\sigma$ refer to the mean and standard deviation in prediction accuracy. $\Delta$ refers to the difference in cloud classification performance with respect to the \textbf{NL} scenario.}\label{table:expts:mc}
%	\centering 
%	\begin{adjustbox}{max width=\textwidth}
%		\begin{tabular}{|c|c|ccc|ccc|ccc|ccc|}
%			\hline
%			\gls{pb} &   $\mu_{\textbf{NL}}$ &   $\mu_{\textbf{NC}}$ &  $\sigma_{\textbf{NC}}$ &  $\Delta_{\textbf{NC}}$ &  silrtc\_mean &  silrtc\_std &  silrtc\_del &  halrtc\_mean &  halrtc\_std &  halrtc\_del &  caltec\_mean &  caltec\_std &  caltec\_del \\
%			\hline
%			              0.01 &  0.853741 &  0.847052 &  0.000000 & -0.006689 &     0.827243 &    0.010761 &   -0.026498 &     0.853272 &    0.001073 &   -0.000470 &     0.852834 &    0.000884 &   -0.000907 \\
%			           0.10 &  0.853741 &  0.748089 &  0.262601 & -0.105653 &     0.732232 &    0.005555 &   -0.121510 &     0.834807 &    0.002116 &   -0.018934 &     0.834257 &    0.002948 &   -0.019485 \\
%			              0.20 &  0.853741 &  0.554956 &  0.005105 & -0.298785 &     0.655134 &    0.009329 &   -0.198607 &     0.785407 &    0.007076 &   -0.068335 &     0.787059 &    0.005938 &   -0.066683 \\
%			              0.30 &  0.853741 &  0.341804 &  0.006846 & -0.511937 &     0.531730 &    0.009741 &   -0.322012 &     0.681924 &    0.009048 &   -0.171817 &     0.696615 &    0.010313 &   -0.157127 \\
%			\hline
%		\end{tabular}
%		
%	\end{adjustbox}
%\end{table}

Table \ref{table:expts:mc:0.3} summarizes the quantitative results with Gilbert-Elliott channels with loss probability of 30\%. The highest prediction accuracy with completed tensors are highlighted in green.

\begin{table}[H]
	\caption[ResNet18 Monte Carlo results with 30 \% loss probability]{\gls{resnet18} \addone~Monte Carlo results for loss probability $\gls{pb} = 30\%$. \textbf{NL} refers to the scenario with no quantization and no Gilbert-Elliott channel. \textbf{NC} refers to the scenario with the Gilbert-Elliot channel and quantization but with no tensor completion. $\mu$ and $\sigma$ refer to the mean and standard deviation in prediction accuracy.} \label{table:expts:mc:0.3}
	\centering
	\begin{adjustbox}{max width = \textwidth}
		\begin{tabular}{|c|c|cc|cc|cc|cc|}
			\hline
			\gls{bl} &  \textbf{NL} &  $\mu_{\textbf{NC}}$ &  $\sigma_{\textbf{NC}}$ &  $\mu_{\text{\gls{silrtc}}}$ &  $\sigma_{\text{\gls{silrtc}}}$ &  $\mu_{\text{\gls{halrtc}}}$ &  $\sigma_{\text{\gls{halrtc}}}$ &  $\mu_{\text{\gls{caltec}}}$ &  $\sigma_{\text{\gls{caltec}}}$ \\
			\hline \hline 
			1 &   0.853741 &             0.329819 &            0.009287 &     0.542744 &    0.010268 &     0.695465 &    0.012182 &    \cellcolor{green!25} 0.711451 &    0.007660 \\
			2 &   0.853741 &             0.343878 &            0.011190 &     0.544218 &    0.010708 &     0.690816 &    0.007774 &     \cellcolor{green!25}0.707937 &    0.009462 \\
			3 &   0.853741 &             0.334921 &            0.008036 &     0.539456 &    0.008062 &     0.687755 &    0.011111 &     \cellcolor{green!25} 0.703968 &    0.008730 \\
			4 &   0.853741 &             0.340590 &            0.007809 &     0.527098 &    0.012882 &     0.680045 &    0.007803 &     \cellcolor{green!25} 0.693084 &    0.008885 \\
			5 &   0.853741 &             0.344104 &            0.011215 &     0.527098 &    0.015591 &     0.677211 &    0.011956 &     \cellcolor{green!25} 0.690703 &    0.009790 \\
			6 &   0.853741 &             0.348526 &            0.011807 &     0.525397 &    0.014154 &     0.673923 &    0.013767 &     \cellcolor{green!25} 0.682993 &    0.008288 \\
			7 &   0.853741 &             0.350794 &            0.012546 &     0.516100 &    0.016530 &     0.668254 &    0.013683 &     \cellcolor{green!25} 0.686168 &    0.007770 \\
			\hline 
		\end{tabular}%
	\end{adjustbox}
\end{table}

A relatively small number of Monte Carlo experiments (10) were run for each average burst length. Therefore, a statistical significance test, Welch's $t$-test, for estimates with unequal variance was run.  Table shows the $p$-values for each combination of the \textbf{NC} case and a tensor completion case.

\begin{table}[H]
	\caption[Statistical significance results for the 30\% burst loss scenario with no tensor completion]{Statistical significance test results for the 30\% burst loss Monte Carlo experiments. \textbf{NC} refers to the scenario with the Gilbert-Elliott channel and quantization but with no tensor completion. \textbf{TC} refers to the scenario in which corrupted tensors are repaired with a tensor completion method. $p$-values for two-sided $t$-test for each pair $\{\textbf{NC},\textbf{TC}\}$.} \label{table:expts:ttest:gc}
	\centering
	\begin{adjustbox}{max width = \textwidth}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\multicolumn{1}{|c|}{} &
			\multicolumn{3}{c|}{\textbf{NC}} \\
			\gls{pb} &  \gls{silrtc} &  \gls{halrtc} &  \gls{caltec} \\
			\hline \hline 
			1 &  $5.372694 \times 10^{-20}$ &  $2.397729 \times 10^{-22}$ &  $4.713118 \times 10^{-25}$ \\
			2 &  $8.872425 \times 10^{-19}$ &  $5.485811 \times 10^{-22}$ &  $2.274292 \times 10^{-23}$ \\
			3 &  $2.364091 \times 10^{-21}$ &  $1.957926 \times 10^{-22}$ &  $1.732656 \times 10^{-25}$ \\
			4 &  $4.798196 \times 10^{-16}$ & $1.543322 \times 10^{-25}$ &  $5.760696 \times 10^ {-25}$ \\
			5 &  $2.171108 \times 10^{-15}$ &  $3.068924 \times 10^{-22}$ &  $4.840899 \times 10^{-23}$ \\
			6 &  $3.787578 \times 10^{-16}$ &  $5.676298 \times 10^{-21}$ &  $1.974453 \times 10^{-21}$ \\
			7 &  $2.124888 \times 10^{-14}$ &  $7.509839e \times 10^{21}$ &  $3.920962 \times 10^{-20}$ \\
			\hline
		\end{tabular}%
		\end{adjustbox}
\end{table}

As can be seen in Table \ref{table:expts:ttest:gc}, all $p$-values were less than 0.05 (5\% probability), which means that the difference in estimates is statistically significant. This means that there is evidence, over 10 Monte Carlo runs for each burst length, that tensor completion methods bring a statistically significant improvement in cloud classification performance. Similarly, Welch's $t$-test is run between pairs of tensor completion methods. Table \ref{table:expts:ttest:tc} shows the $p$-values for each pair of tensor completion methods.

\begin{table}[H]
	\caption[Statistical significance results for the 30\% burst loss scenario with no tensor completion]{Statistical significance test results for the 30\% burst loss Monte Carlo experiments with pairs of tensor completion methods. $p$-values for two-sided $t$-test for each pair $\{\textbf{TC} \mbox{ } \text{method 1},\textbf{TC} \mbox{ } \text{method 2}\}$.} \label{table:expts:ttest:tc}
	\centering
	\begin{adjustbox}{max width = \textwidth}
		\begin{tabular}{|c|cc|c|}
			\hline
			\multicolumn{1}{|c|}{} &
			\multicolumn{2}{c|}{\gls{caltec}} & \multicolumn{1}{c|}{\gls{halrtc}} \\
			\gls{bl} &  \gls{silrtc} &  \gls{halrtc} &  \gls{silrtc} \\
			\hline \hline 
				1 &   $6.794816 \times 10^{-18}$ &       0.004489 &   $3.532067 \times 10^{-16}$ \\
				2 &   $1.119562 \times 10^{-17}$ &       0.000586 &   $1.690975 \times 10^{-16}$ \\
				3 &   $3.058285 \times 10^{-19}$ &       0.003101 &   $2.562640 \times 10^{-16}$ \\
				4 &   $6.972254 \times 10^{-16}$ &       0.003981 &   $8.845756 \times 10^{-15}$ \\
				5 &   $3.834074 \times 10^{-14}$ &       0.017756 &   $3.787183 \times 10^{-14}$ \\
				6 &   $3.162889 \times 10^{-14}$ &      \cellcolor{red!40} 0.111374 &   $1.206839 \times 10^{-14}$ \\
				7 &   $7.573049 \times 10^{-13}$ &       0.004086 &   $6.803447 \times 10^{-14}$ \\
				\hline
			\end{tabular}%
	\end{adjustbox}
\end{table}

As can be seen in Table \ref{table:expts:ttest:tc}, all $p$-values were less than 0.05, except for  $\{\text{\gls{caltec}},\text{\gls{halrtc}}\}$ for an average burst length of six packets (highlighted in red). Therefore, there is evidence that, over 10 Monte Carlo runs, there is a statistical difference in the cloud prediction accuracy obtained with tensor completion methods, except for the pair $\{\text{\gls{caltec}},\text{\gls{halrtc}}\}$ in the $\gls{bl}=6$ case. For the other six average burst lenths, for a burst loss probability of 30\%, there is statistically significant evidence that \gls{caltec} outperforms \gls{silrtc} and \gls{halrtc} (c.f. Table \ref{table:expts:mc:0.3}).

\textbf{Monte Carlo results with other loss probabilities}

Table \ref{table:expts:mc:0.2} summarizes the quantitative results with Gilbert-Elliott channels with loss probability of 30\%. The highest prediction accuracy with tensor completion methods are highlighted in green.

\begin{table}[H]
	\caption[ResNet18 Monte Carlo results with 20 \% loss probability]{\gls{resnet18} \addone~Monte Carlo results for loss probability $\gls{pb} = 20\%$. \textbf{NL} refers to the scenario with no quantization and no Gilbert-Elliott channel. \textbf{NC} refers to the scenario with the Gilbert-Elliot channel and quantization but with no tensor completion. $\mu$ and $\sigma$ refer to the mean and standard deviation in prediction accuracy.} \label{table:expts:mc:0.2}
	\centering
	\begin{adjustbox}{max width = \textwidth}
		\begin{tabular}{|c|c|cc|cc|cc|cc|}
			\hline
			\gls{bl} &  \textbf{NL} &  $\mu_{\textbf{NC}}$ &  $\sigma_{\textbf{NC}}$ &  $\mu_{\text{\gls{silrtc}}}$ &  $\sigma_{\text{\gls{silrtc}}}$ &  $\mu_{\text{\gls{halrtc}}}$ &  $\sigma_{\text{\gls{halrtc}}}$ &  $\mu_{\text{\gls{caltec}}}$ &  $\sigma_{\text{\gls{caltec}}}$ \\
			\hline \hline 
		1 &   0.853741 &             0.561905 &            0.006750 &     0.666213 &    0.016895 &     0.791383 &    0.011292 &   \cellcolor{green!25}  0.795465 &    0.008036 \\
		2 &   0.853741 &             0.558844 &            0.007911 &     0.666100 &    0.013039 &   \cellcolor{green!25}  0.795351 &    0.005765 &     0.792063 &    0.007351 \\
		3 &   0.853741 &             0.554989 &            0.008638 &     0.658050 &    0.016322 &     0.789796 &    0.006844 &  \cellcolor{green!25}   0.790590 &    0.008485 \\
		4 &   0.853741 &             0.554195 &            0.010024 &     0.653401 &    0.009099 &     0.787982 &    0.005531 &   \cellcolor{green!25}  0.788322 &    0.004274 \\
		5 &   0.853741 &             0.557370 &            0.014154 &     0.649433 &    0.010718 &     0.779705 &    0.013997 &  \cellcolor{green!25}    0.785147 &    0.008712 \\
		6 &   0.853741 &             0.544558 &            0.013923 &     0.655556 &    0.011493 &     0.774603 &    0.008173 &   \cellcolor{green!25}  0.779478 &    0.013456 \\
		7 &   0.853741 &             0.552834 &            0.009964 &     0.637188 &    0.012316 &    \cellcolor{green!25} 0.779025 &    0.008214 &     0.778345 &    0.011329 \\
			\hline 
		\end{tabular}
	\end{adjustbox}
\end{table}

\begin{table}[H]
	\caption[Statistical significance results for the 20\% burst loss scenario with no tensor completion]{Statistical significance test results for the 20\% burst loss Monte Carlo experiments with pairs of tensor completion methods. $p$-values for two-sided $t$-test for each pair $\{\textbf{TC} \mbox{ } \text{method 1},\textbf{TC} \mbox{ } \text{method 2}\}$.} \label{table:expts:ttest:tc:0.2}
	\centering
	\begin{adjustbox}{max width = \textwidth}
		\begin{tabular}{|c|cc|c|}
			\hline
			\multicolumn{1}{|c|}{} &
			\multicolumn{2}{c|}{\gls{caltec}} & \multicolumn{1}{c|}{\gls{halrtc}} \\
			\gls{bl} &  \gls{silrtc} &  \gls{halrtc} &  \gls{silrtc} \\
			\hline \hline 
			1 &   $2.856117 \times 10^{-11}$ &      \cellcolor{red!40} 0.389864 &   $4.484050 \times 10^{-12}$ \\
			2 &   $3.361040 \times 10^{-13}$ &   \cellcolor{red!40}    0.305770 &   $2.010178 \times 10^{-12}$ \\
			3 &   $6.945824 \times 10^{-12}$ &  \cellcolor{red!40}      0.829679 &   $3.471161 \times 10^{-11}$ \\
			4 &   $7.484135 \times 10^{-15}$ &  \cellcolor{red!40}     0.885662 &   $3.393214 \times 10^{-16}$ \\
			5 &   $3.213660 \times 10^{-16}$ &    \cellcolor{red!40}   0.337689 &   $6.633064 \times 10^{-14}$ \\
			6 &   $6.802484 \times 10^{-14}$ &  \cellcolor{red!40}     0.367740 &   $1.730071 \times 10^{-14}$ \\
			7 &   $1.893453 \times 10^{-15}$ &    \cellcolor{red!40}   0.885827 &   $5.443340 \times 10^{-15}$ \\
				\hline
		\end{tabular}%
	\end{adjustbox}
\end{table}

Table \ref{table:expts:ttest:tc:0.2} shows the $p$-values for all tensor completion experiments in which the burst loss probability of the Gilbert-Elliott channel was 20\%. For all average burst lengths \gls{bl}, there was a statistically significant difference in the performance of \gls{silrtc} and the other tensor completion method. There was no statistically significant difference in the performance of \gls{caltec} and \gls{halrtc} since the $p$ values always exceeded 5\%.

Table \ref{table1:expts:mc:0.1} summarizes the quantitative results with Gilbert-Elliott channels with loss probability of 10\%. The highest prediction accuracy with tensor completion methods are highlighted in green.

\begin{table}[H]
	\caption[ResNet18 Monte Carlo results with 10 \% loss probability]{\gls{resnet18} \addone~Monte Carlo results for loss probability $\gls{pb} = 10\%$. \textbf{NL} refers to the scenario with no quantization and no Gilbert-Elliott channel. \textbf{NC} refers to the scenario with the Gilbert-Elliot channel and quantization but with no tensor completion. $\mu$ and $\sigma$ refer to the mean and standard deviation in prediction accuracy.} \label{table1:expts:mc:0.1}
	\centering
	\begin{adjustbox}{max width = \textwidth}
		\begin{tabular}{|c|c|cc|cc|cc|cc|}
			\hline
			\gls{bl} &  \textbf{NL} &  $\mu_{\textbf{NC}}$ &  $\sigma_{\textbf{NC}}$ &  $\mu_{\text{\gls{silrtc}}}$ &  $\sigma_{\text{\gls{silrtc}}}$ &  $\mu_{\text{\gls{halrtc}}}$ &  $\sigma_{\text{\gls{halrtc}}}$ &  $\mu_{\text{\gls{caltec}}}$ &  $\sigma_{\text{\gls{caltec}}}$ \\
			\hline \hline 
		1 &   0.853741 &             0.768481 &            0.008765 &     0.723243 &    0.006030 &     0.837528 &    0.004565 &    \cellcolor{green!25} 0.840023 &    0.004316 \\
		2 &   0.853741 &             0.757596 &            0.007131 &     0.726531 &    0.011759 &    \cellcolor{green!25} 0.837642 &    0.006247 &     0.835147 &    0.003698 \\
		3 &   0.853741 &             0.748866 &            0.010603 &     0.728571 &    0.010093 &    \cellcolor{green!25} 0.835034 &    0.006236 &     0.832086 &    0.005056 \\
		4 &   0.853741 &             0.742517 &            0.010360 &     0.735374 &    0.006654 &  \cellcolor{green!25}   0.835488 &    0.005747 &     0.835034 &    0.003735 \\
		5 &   0.853741 &             0.746145 &            0.009838 &     0.736168 &    0.008301 &  \cellcolor{green!25}   0.833560 &    0.005952 &     0.834694 &    0.008141 \\
		6 &   0.853741 &             0.737075 &            0.012741 &     0.737415 &    0.005830 &     0.832540 &    0.007331 &  \cellcolor{green!25}   0.832993 &    0.006612 \\
		7 &   0.853741 &             0.735941 &            0.013352 &     0.738322 &    0.009751 &   \cellcolor{green!25}  0.831859 &    0.006823 &     0.829819 &    0.005005 \\
			\hline 
		\end{tabular}
	\end{adjustbox}
\end{table}

Table \ref{table1:expts:mc:0.1} summarizes the quantitative results with Gilbert-Elliott channels with loss probability of 10\%. The highest prediction accuracy with tensor completion methods are highlighted in green.

\begin{table}[H]
	\caption[Statistical significance results for the 10\% burst loss scenario with no tensor completion]{Statistical significance test results for the 10\% burst loss Monte Carlo experiments with pairs of tensor completion methods. $p$-values for two-sided $t$-test for each pair $\{\textbf{TC} \mbox{ } \text{method 1},\textbf{TC} \mbox{ } \text{method 2}\}$.} \label{table:expts:ttest:tc:0.1}
	\centering
	\begin{adjustbox}{max width = \textwidth}
		\begin{tabular}{|c|cc|c|}
			\hline
			\multicolumn{1}{|c|}{} &
			\multicolumn{2}{c|}{\gls{caltec}} & \multicolumn{1}{c|}{\gls{halrtc}} \\
			\gls{bl} &  \gls{silrtc} &  \gls{halrtc} &  \gls{silrtc} \\
			\hline \hline 
	1 &   $7.070258 \times 10^{-19}$ &  \cellcolor{red!40}     0.249105 &   $5.618042 \times 10^{-19}$ \\
	2 &   $3.883143 \times 10^{-11}$ &    \cellcolor{red!40}   0.319413 &   $7.733244 \times 10^{-13}$ \\
	3 &   $4.437797 \times 10^{-13}$ &    \cellcolor{red!40}   0.285781 &   $4.117885 \times 10^{-14}$ \\
	4 &   $7.663284 \times 10^{-16}$ &   \cellcolor{red!40}    0.845229 &   $1.478984 \times 10^{-17}$ \\
	5 &   $1.493439 \times 10^{-15}$ &      \cellcolor{red!40} 0.740170 &   $2.240631 \times 10^{-15}$ \\
	6 &   $2.977022 \times 10^{-17}$ &  \cellcolor{red!40}     0.891934 &  $2.304421 \times 10^{-16}$ \\
	7 &   $1.145549 \times 10^{-12}$ &   \cellcolor{red!40}    0.479470 &   $6.449025 \times 10^{-14}$ \\
			\hline
		\end{tabular}%
	\end{adjustbox}
\end{table}

Table \ref{table:expts:ttest:tc:0.1} shows the $p$-values for all tensor completion experiments in which the burst loss probability of the Gilbert-Elliott channel was 10\%. For all average burst lengths \gls{bl}. there was a statistically significant difference in the cloud prediction accuracies with \gls{silrtc} compared to the performance achieved with \gls{caltec} and \gls{halrtc}. The difference in performance between \gls{caltec} and \gls{halrtc} was not statistically significant, given that the corresponding $p$-values exceeded 5\%.

\begin{table}[H]
	\caption[ResNet18 Monte Carlo results with 1\% loss probability]{\gls{resnet18} \addone~Monte Carlo results for loss probability $\gls{pb} = 1\%$. \textbf{NL} refers to the scenario with no quantization and no Gilbert-Elliott channel. \textbf{NC} refers to the scenario with the Gilbert-Elliot channel and quantization but with no tensor completion. $\mu$ and $\sigma$ refer to the mean and standard deviation in prediction accuracy.} \label{table:expts:mc:0.01}
	\centering
	\begin{adjustbox}{max width = \textwidth}
		\begin{tabular}{|c|c|cc|cc|cc|cc|}
			\hline
			\gls{bl} &  \textbf{NL} &  $\mu_{\textbf{NC}}$ &  $\sigma_{\textbf{NC}}$ &  $\mu_{\text{\gls{silrtc}}}$ &  $\sigma_{\text{\gls{silrtc}}}$ &  $\mu_{\text{\gls{halrtc}}}$ &  $\sigma_{\text{\gls{halrtc}}}$ &  $\mu_{\text{\gls{caltec}}}$ &  $\sigma_{\text{\gls{caltec}}}$ \\
			\hline \hline 
			1 &   0.853741 &             0.846485 &            0.005808 &     0.803741 &    0.009560 &   \cellcolor{green!25}  0.852381 &    0.002814 &     0.851701 &    0.003855 \\
			2 &   0.853741 &             0.846939 &            0.003928 &     0.822336 &    0.005415 &   \cellcolor{green!25}  0.854082 &    0.002871 &     0.852381 &    0.002139 \\
			3 &   0.853741 &             0.846825 &            0.005656 &     0.826531 &    0.007656 &     0.852494 &    0.003707 & \cellcolor{green!25}     0.853968 &    0.002991 \\
			4 &   0.853741 &             0.848073 &            0.003760 &     0.832086 &    0.004604 &     0.852721 &    0.003019 &  \cellcolor{green!25}   0.853175 &    0.002392 \\
			5 &   0.853741 &             0.847392 &            0.004973 &     0.836054 &    0.004842 &  \cellcolor{green!25}   0.854875 &    0.003166 &     0.853741 &    0.003000 \\
			6 &   0.853741 &             0.846259 &            0.004708 &     0.832426 &    0.004144 & \cellcolor{green!25}    0.851927 &    0.003733 &     0.851587 &    0.002657 \\
			7 &   0.853741 &             0.847392 &            0.003484 &     0.837528 &    0.005021 & \cellcolor{green!25}    0.854422 &    0.003008 &     0.853288 &    0.002442 \\
			\hline 
		\end{tabular}
	\end{adjustbox}
\end{table}


\begin{table}[H]
	\caption[Statistical significance results for the 10\% burst loss scenario with no tensor completion]{Statistical significance test results for the 10\% burst loss Monte Carlo experiments with pairs of tensor completion methods. $p$-values for two-sided $t$-test for each pair $\{\textbf{TC} \mbox{ } \text{method 1},\textbf{TC} \mbox{ } \text{method 2}\}$.} \label{table:expts:ttest:tc:0.01}
	\centering
	\begin{adjustbox}{max width = \textwidth}
		\begin{tabular}{|c|cc|c|}
			\hline
			\multicolumn{1}{|c|}{} &
			\multicolumn{2}{c|}{\gls{caltec}} & \multicolumn{1}{c|}{\gls{halrtc}} \\
			\gls{bl} &  \gls{silrtc} &  \gls{halrtc} &  \gls{silrtc} \\
			\hline \hline 
			1 &   $1.022522 \times 10^{-8}$ &      \cellcolor{red!40} 0.674479 &   $2.398399 \times 10^{-8}$ \\
			2 &   $3.582450 \times 10^{-9}$ &   \cellcolor{red!40}    0.172597 &   $4.375549 \times 10^{-10}$ \\
			3 &   $4.416115 \times 10^{-7}$ &    \cellcolor{red!40}   0.366070 &   $4.950577 \times 10^{-7}$ \\
			4 &   $1.115009 \times 10^{-8}$ &    \cellcolor{red!40}   0.728219 &   $7.226725 \times 10^{-9}$ \\
			5 &   $1.245014 \times 10^{-7}$ &  \cellcolor{red!40}     0.445664 &   $5.110022\times10^{-8}$ \\
			6 &   $4.932438 \times 10^{-9}$ &    \cellcolor{red!40}   0.826532 &   $4.763071 \times 10^{-9}$ \\
			7 &   $1.172156 \times 10^{-6}$ &    \cellcolor{red!40}   0.392096 &   $3.690091 \times 10^{-7}$ \\
			\hline
		\end{tabular}%
	\end{adjustbox}
\end{table}

Table \ref{table:expts:ttest:tc:0.01} shows the $p$-values for all tensor completion experiments in which the burst loss probability of the Gilbert-Elliott channel was 1\%. For all average burst lengths \gls{bl}, there was a statistically significant difference in the cloud prediction accuracies with \gls{silrtc} compared to the performance achieved with \gls{caltec} and \gls{halrtc}. The difference in performance between \gls{caltec} and \gls{halrtc} was not statistically significant, given that the corresponding $p$-values exceeded 5\%.

According to the results summarized in Tables \ref{table:expts:mc:0.2}, \ref{table1:expts:mc:0.1} and \ref{table:expts:mc:0.01}, the best performing tensor completion method was either \gls{halrtc} or \gls{caltec}. Over all average burst lengths for a loss probability of 30\%, \gls{caltec} gave the best inference accuracy. Moreover, \gls{caltec} experiments had the smallest impact on inference latency. \gls{caltec}'s execution time is proportional to the number of missing packets to `fill in'. When there are very few lost packets (loss probability of 1\% and 10\%), \gls{caltec} takes, on average, less than 6 minutes to process one Monte Carlo run worth of damaged \gls{resnet18} tensors. On the other hand, \gls{silrtc} and \gls{halrtc}, being iterative procedures with no stopping condition, have execution times which are not dependent on the number of missing packets to repair.

Table \ref{table:expts:mc:long} presents the Monte Carlo experiment results summarized over all average burst lengths. For each loss probability, the prediction accuracy over all burst lengths were accumulated and used to calculate the mean prediction accuracy ($\mu$) and the standard deviation in prediction accuracy ($\sigma$). This implies that results for a given value of loss probability are summarizing 70 Monte Carlo runs worth of simulations. The difference $\Delta$ between the best case prediction accuracy and the `completed' tensor prediction accuracy signifies the relative performance of each tensor completion method.

\begin{table}[H]
	\caption[Summarized ResNet18 Monte Carlo results with tensor completion]{\gls{resnet18} \addone~Monte Carlo results for each loss probability $\gls{pb}$ over all burst lengths $\gls{bl}$ under default settings. \textbf{NL} refers to the scenario with no packet loss and no quantization. \textbf{NC} refers to the scenario with the Gilbert-Elliott channel and quantization but with no tensor completion. \textbf{TC} refers to the scenario with the Gilbert-Elliott channel and quantization followed by tensor completion. $\mu$ and $\sigma$ refer to the mean and standard deviation in prediction accuracy. $\Delta$ refers to the difference in cloud classification performance with respect to the \textbf{NL} scenario.}\label{table:expts:mc:long}
	\centering 
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{|c|c|c|ccc|ccc|}
			\hline 
			\gls{pb} & \textbf{Method} &   $\mu_{\textbf{NL}}$ &   $\mu_{\textbf{NC}}$ &  $\sigma_{\textbf{NC}}$ &  $\Delta_{\textbf{NC}}$ & $\mu_{\textbf{TC}}$ & $\sigma_{\textbf{TC}}$ & $\Delta_{\textbf{TC}}$ \\
			\hline \hline
			$0.01$ & \gls{silrtc} & 0.853741 &  0.847052 &  0.000000 & -0.006689 & 0.827243 &    0.010761 &   -0.026498 \\
			$0.01$ & \cellcolor{green!25}\gls{halrtc} & 0.853741 &  0.847052 &  0.000000 & -0.006689 & \cellcolor{green!25}0.853272 &    0.001073 &   \cellcolor{green!25}-0.000470 \\
			$0.01$ & \gls{caltec} & 0.853741 &  0.847052 &  0.000000 &  -0.006689 & 0.852834 &    0.000884 &   -0.000907 \\
			\hline 
			0.10 & \gls{silrtc} &  0.853741 &  0.748089 &  0.262601 & -0.105653 &    0.732232 &    0.005555 &   -0.121510 \\
			0.10 & \cellcolor{green!25}\gls{halrtc} & 0.853741 &  0.748089 &  0.262601 & -0.105653 & \cellcolor{green!25}0.834807 &    0.002116 &   \cellcolor{green!25}-0.018934 \\
			0.10 & \gls{caltec} & 0.853741 &  0.748089 &  0.262601 & -0.105653 & 0.834257 &    0.002948 &   -0.019485 \\
			\hline
			0.20 & \gls{silrtc} & 0.853741 &  0.554956 &  0.005105 & -0.298785 &     0.655134 &    0.009329 &   -0.198607\\
			0.20 & \gls{halrtc} & 0.853741 &  0.554956 &  0.005105 & -0.298785 &  0.785407 &    0.007076 &   -0.068335\\
			0.20 & \cellcolor{green!25}\gls{caltec} & 0.853741 &  0.554956 &  0.005105 & -0.298785 & \cellcolor{green!25}0.787059 &    0.005938 &   \cellcolor{green!25}-0.066683 \\
			\hline
			0.30 & \gls{silrtc} &  0.853741 &  0.341804 &  0.006846 & -0.511937 &     0.531730 &    0.009741 &   -0.322012 \\
			0.30 & \gls{halrtc} & 0.853741 &  0.341804 &  0.006846 & -0.511937 & 0.681924 &    0.009048 &   -0.171817 \\
			0.30 & \cellcolor{green!25}\gls{caltec} & 0.853741 &  0.341804 &  0.006846 & -0.511937 & \cellcolor{green!25}0.696615 &    0.010313 &  \cellcolor{green!25} -0.157127  \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}

\gls{halrtc} slightly outperforms \gls{caltec} when the probability of a burst loss is low ($\gls{pb} = 0.01, 0.1$). The difference in mean classification accuracy is very small (less than 1\%). At higher loss probabilities of 20 and 30 percent, \gls{caltec} gives the best performance and manages to get a 1.47\% improvement over \gls{halrtc} at $\gls{pb}= 30\%$.

\begin{table}[H]
	\caption[Statistical significance test results for all Monte Carlo experiments accumulated according to loss probability]{Statistical significance test results for all Monte Carlo experiments accumulated according to loss probability $\gls{pb}$ with pairs of tensor completion methods. $p$-values for two-sided $t$-test for each pair  $\{\textbf{TC} \mbox{ } \text{method 1},\textbf{TC} \mbox{ } \text{method 2}\}$.} \label{table:expts:mc:all}
	\centering
		\begin{adjustbox}{max width = \textwidth}
		\begin{tabular}{|c|cc|c|}
			\hline
			\multicolumn{1}{|c|}{} &
			\multicolumn{2}{c|}{\gls{caltec}} & \multicolumn{1}{c|}{\gls{halrtc}} \\
			\gls{pb} &  \gls{silrtc} &  \gls{halrtc} &  \gls{silrtc} \\
			\hline \hline 
		0.01 &     $7.327862 \times 10^{-35}$ &    \cellcolor{red!40} $4.208008 \times 10^{-1}$ &     $3.210643 \times 10^{-35}$ \\
		0.1 &    $2.268893 \times 10^{-110}$ &   \cellcolor{red!40} $6.119328 \times 10^{-1}$ &   $8.579081 \times 10^{-110}$ \\
		0.2 &     $4.582257 \times 10^{-97}$ &    \cellcolor{red!40} $3.882664 \times 10^{-1}$ &     $1.634535 \times 10^{-95}$ \\
		0.3 &    $2.057486 \times 10^{-105}$ &     $7.982438 \times 10^{-9}$ &     $4.233786 \times 10^{-98}$ \\
			\hline
		\end{tabular}%
	\end{adjustbox}
\end{table}

Table \ref{table:expts:mc:all} shows the $p$-values for all tensor completion completion experiments. Monte Carlo results across the seven burst lengths were accumulated for each value of loss probability \gls{pb} investigated. As shown by the highlighted cells in Table \ref{table:expts:mc:all}, there is no statistically significant difference in the performance of \gls{caltec} and \gls{halrtc} for $\gls{pb}=\{0.01,0.1,0.2\}$ over ten Monte Carlo runs for each burst length. 


\begin{table}[H]
	\caption[Summarized ResNet18 Monte Carlo results with tensor completion]{\gls{resnet18} \addthree~Monte Carlo results for each loss probability $\gls{pb}$ over all burst lengths $\gls{bl}$ under default settings. \textbf{NL} refers to the scenario with no packet loss and no quantization. \textbf{NC} refers to the scenario with the Gilbert-Elliott channel and quantization but with no tensor completion. \textbf{TC} refers to the scenario with the Gilbert-Elliott channel and quantization followed by tensor completion. $\mu$ and $\sigma$ refer to the mean and standard deviation in prediction accuracy. $\Delta$ refers to the difference in cloud classification performance with respect to the \textbf{NL} scenario.}\label{table:expts:mc:long:add3}
	\centering 
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{|c|c|c|ccc|ccc|}
			\hline 
			\gls{pb} & \textbf{Method} &   $\mu_{\textbf{NL}}$ &   $\mu_{\textbf{NC}}$ &  $\sigma_{\textbf{NC}}$ &  $\Delta_{\textbf{NC}}$ & $\mu_{\textbf{TC}}$ & $\sigma_{\textbf{TC}}$ & $\Delta_{\textbf{TC}}$ \\
			\hline \hline
			$0.01$ & \gls{silrtc} & 0.853741 &  0.851474 &  0.001560 & -0.002268 & 0.665695 &    0.056755 &   -0.188047 \\
			$0.01$ & \cellcolor{green!25}\gls{halrtc} & 0.853741 &  0.851474 &  0.001560 & -0.002268 & \cellcolor{green!25}0.853871 &    0.001068 &  \cellcolor{green!25}  0.000130 \\
			$0.01$ & \gls{caltec} & 0.853741 &  0.851474 &  0.001560 &  -0.002268 & 0.853207 &    0.000962 &   -0.000534 \\
			\hline 
			0.10 & \gls{silrtc} &  0.853741 &  0.817444 &  0.000825 & -0.036297 & 0.388889 & 0.006828 & -0.464853 \\
			0.10 & \cellcolor{green!25}\gls{halrtc} & 0.853741 &  0.817444 &  0.000825 & -0.036297 & \cellcolor{green!25} 0.843181 &    0.001665 & \cellcolor{green!25}  -0.010560 \\
			0.10 & \gls{caltec} & 0.853741 &  0.817444 &  0.000825 & -0.036297 & 0.834257 &    0.002948 &   -0.019485 \\
			\hline
			0.20 & \gls{silrtc} & 0.853741 &  0.757208 &  0.001844 & -0.096534 &  0.328523 &    0.004150 &   -0.525219\\
			0.20 & \cellcolor{green!25}\gls{halrtc} & 0.853741 &  0.757208 &  0.001844 & -0.096534 & \cellcolor{green!25} 0.818756 &    0.002659 & \cellcolor{green!25}  -0.034985\\
			0.20 & \gls{caltec} & 0.853741 &  0.757208 &  0.001844 & -0.096534 &  0.806900 &    0.003129 &   -0.046842 \\
			\hline
			0.30 & \gls{silrtc} &  0.853741 &  0.654098 &  0.006637 & -0.199644 &   0.266440 &    0.004717 &   -0.587302 \\
			0.30 & \cellcolor{green!25} \gls{halrtc} & 0.853741 &  0.654098 &  0.006637 & -0.199644 & \cellcolor{green!25} 0.767946 &    0.003650 &  \cellcolor{green!25} -0.085795 \\
			0.30 & \gls{caltec} & 0.853741 &  0.654098 &  0.006637 & -0.199644 &  0.761257 &    0.003647 &   -0.092485  \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}



\section{Experiments under speed-matched execution} \label{sec:expts:speed}

The proposed method \gls{caltec} processes corrupted tensors much faster than \gls{silrtc} and \gls{halrtc}. To further explore the relative strengths of these methods, it is worthwhile to run Monte Carlo experiments in which \gls{caltec} sets a target execution time to process a corrupted tensor for a particular image. Then iterations of the other tensor completion methods are executed until the time limit set by \gls{caltec} on the very same corrupted tensor is reached. These `speed-matched' experiments (as reported in \cite{9017944}) are an indication of inference latency delay in an image classification \gls{ci} application.

In this set of experiments, 8-bit quantized ~\addone~deep feature tensors are packetized and transmitted over a Gilbert-Elliott channel. At the cloud service, the received packets are inverse quantized and repaired by \gls{caltec}. The time taken by \gls{caltec} for a certain corrupted tensor serves as execution time limit in a \verb|while| loop implementing iterations of \gls{silrtc} or \gls{halrtc}. \gls{caltec} took less than one second to process a corrupted tensor for a test image. In this short time span, \gls{dfts}'s \gls{silrtc} and \gls{halrtc} methods were only able to execute a single iteration.

Figure \ref{fig:expts:speed} shows the Monte Carlo results for Gilbert-Elliott channels with a burst loss probability of $30\%$. Fifty Monte Carlo runs were simulated for each burst length. 

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{Figures/lp03speedmatched.pdf}
	\caption[Speed-matched Monte Carlo results with completed ResNet18 tensors]{Speed-matched Monte Carlo results with completed \addone~tensors. \gls{silrtc} produces similar prediction accuracies as the \textbf{NC} (No Channel) case.}
	\label{fig:expts:speed}
\end{figure}

\gls{caltec} produced the highest prediction accuracy at every burst length. The \gls{caltec} execution time limited the other tensor completion to a single iteration. After one single iteration of \gls{silrtc}, the completed tensors lead to cloud prediction accuracies which are virtually identical to the \textbf{NC} (no tensor completion) scenario. \gls{halrtc}, on the other hand, leads to cloud prediction accuracies which are very close to 0\%. \gls{halrtc}'s poorer performance is explained by the fact that the first iteration of \gls{halrtc} leads to a completed tensor which is very different from the original tensor. This is supported by Figure \ref{fig:expts:maxiters} in which the initial Frobenius norm of the difference tensor is at least 10 times larger for \gls{halrtc} than for \gls{silrtc}.

\begin{table}[H]
	\centering
	\caption[Summarized ResNet18 Monte Carlo results with speed-matched tensor completion]{\gls{resnet18} \addone~Monte Carlo results for each loss probability $\gls{pb}$ over all burst lengths $\gls{bl}$ under speed-matched settings. \textbf{NL} refers to the scenario with no packet loss and no quantization. \textbf{NC} refers to the scenario with the Gilbert-Elliott channel and quantization but with no tensor completion. \textbf{TC} refers to the scenario with the Gilbert-Elliott channel and quantization followed by tensor completion. $\mu$ and $\sigma$ refer to the mean and standard deviation in prediction accuracy. $\Delta$ refers to the difference in cloud classification performance with respect to the \textbf{NL} scenario.}\label{table:expts:mc:speed}
	\begin{adjustbox}{max width = \textwidth}
		\begin{tabular}{|c|c|cc|cc|cc|cc|}
			\hline
			\gls{pb} &   $\mu_{\textbf{NL}}$ &   $\mu_{\textbf{NC}}$ &    $\sigma_{\text{NC}}$ &  $\mu_{\text{\gls{silrtc}}}$ &  $\sigma_{\text{\gls{silrtc}}}$ &  $\mu_{\text{\gls{halrtc}}}$ &  $\sigma_{\text{\gls{halrtc}}}$ &  $\mu_{\text{\gls{caltec}}}$ &  $\sigma_{\text{\gls{caltec}}}$\\
			\hline \hline
			0.01 &  0.853741 &  0.847188 &  0.001294 &     0.846709 &    0.001060 &     0.247622 &    0.149237 &    \cellcolor{green!25} 0.853090 &    0.000291 \\
			0.1 &  0.853741 &  0.748005 &  0.009674 &     0.747590 &    0.009672 &     0.001927 &    0.000439 &    \cellcolor{green!25} 0.833029 &    0.001939 \\
			0.2 &  0.853741 &  0.553994 &  0.003844 &     0.553822 &    0.003891 &     0.002614 &    0.000491 &    \cellcolor{green!25} 0.787425 &    0.005619 \\
			0.3 &  0.853741 &  0.339637 &  0.006553 &     0.339605 &    0.006515 &     0.002825 &    0.000439 &     \cellcolor{green!25}0.697162 &    0.008407 \\
			\hline
		\end{tabular}
		\end{adjustbox}
\end{table}


\section{Summary} \label{sec:expts:summary}

This chapter reported on experiments performed in this project. It investigated the effect of quantization and Gilbert-Elliott channel parameters on the cloud prediction accuracy with \gls{vgg16} and \gls{resnet18} tensors. Tensor completion experiments were run with two methods from the literature (\gls{silrtc} and \gls{halrtc}) and a proposed method (\gls{caltec}). Quantitative results are presented to benchmark the performance of this proposed method against those of the general tensor completion methods \gls{silrtc} and \gls{halrtc}. In high burst loss probability scenarios, the proposed method \gls{caltec} leads to the best cloud prediction accuracy. Moreover, \gls{caltec} had the shortest execution times in all Monte Carlo experiments. Monte Carlo experiments under speed-matched settings are run to remove the aspect of execution time from the experimental results. \gls{caltec} vastly outperforms both \gls{silrtc} and \gls{halrtc} under speed-matched settings because only one iteration of these two methods can be executed within the time limit imposed by \gls{caltec}'s execution speed.
